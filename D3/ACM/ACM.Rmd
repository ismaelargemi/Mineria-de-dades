---
output: pdf_document
header-includes:
- \usepackage{fullpage} 
- \usepackage[spanish]{babel}
- \setlength{\headsep}{7mm} 
- \usepackage[linktoc=page]{hyperref}
- \usepackage{fancyhdr}
- \usepackage{floatrow}
- \floatsetup[figure]{capposition=top}
- \floatsetup[table]{style=plaintop}
- \usepackage{float}
- \floatplacement{figure}{H}
- \newcommand{\beginsupplement}{
  \setcounter{table}{23}  
  \renewcommand{\thetable}{\arabic{table}} 
  \setcounter{figure}{62} 
  \renewcommand{\thefigure}{\arabic{figure}}}
- \setlength{\headheight}{13.6pt}
- \setlength{\topmargin}{-10mm}
- \rhead{Minería de Datos}
- \lhead{Entrega D3}
---

\pagenumbering{arabic}
\pagestyle{fancy}

```{=tex}
\cfoot{\thepage}
\setcounter{page}{78}
```
\beginsupplement


# Análisis de correspondencias múltiples (ACM)

```{r, include=FALSE}
library(FactoMineR)
#install.packages("factoextra")
library(factoextra)
library(Matrix)
library(ggplot2)
library(dplyr) 
library(kableExtra)
```

```{r, include=FALSE, echo= FALSE}
load("Dades preprocessades.Rdata")

#mydata contiene toda la base de datos
mydata <- df_preprocessed
mydata[,c("AMT_INCOME_TOTAL","AMT_CREDIT","AMT_ANNUITY","AMT_GOODS_PRICE",
          "DAYS_BIRTH","TARGET","log_AMT_GOODS_PRICE","log_AMT_ANNUITY",
          "DIFF_CREDIT_GOODS")] <- NULL
dd <- mydata

```


El Multiple Correspondance Analysis, ACM en adelante, es un método de análisis factorial para variables categóricas que permite analizar relaciones entre variables, así como reducir la dimensionalidad de la base de datos seleccionando sólo aquellas variables relevantes. Para realizarlo se deben escoger unas variables activas y otras de complementarias. En este caso, como el número de variables es relativamente bajo y se consideran todas relevantes, no se considerará ninguna variable complementaria. Además, se añadirán al análisis las variables numéricas como variables suplementarias, aunque solamente aquellas que se han considerado relevantes en el ACP. 

Consideramos hacer una nueva codificación de las variables, reduciendo su longitud, de tal manera que los resultados obtenidos para el análisis se observen más claramente:

<!-- CAMBIAR EL NOMBRE DE LAS VARIABLES: -->
```{r, echo=FALSE, include=FALSE}
# Definimos el nombre de las nuevas variables

new_variable_names <- c(
  "GDR", "INC_TYPE", "EDU_TYPE", "FAM_STAT", "OCC_TYPE", 
  "ORG_TYPE", "RR_CLIENT", "CAR_AGE", "FAM_MEMBERS", "INC_AMT", "CREDIT_AMT", "AGE_YEARS", "CREDIT_INCOME", "ANNUTY_CREDIT", "DTI_RATIO"
)


colnames(dd) <- new_variable_names
```


## Desarrollo del ACM

```{r, echo=F, include=FALSE}
res.mca <- MCA(dd, quanti.sup = c(8:15), method = "Burt")
```

```{r, warning=FALSE, echo=F,fig.cap = "Correlación entre Variables y Dimensiones Principales", fig.show='hold',out.width="75%",out.height="75%"}
plot(res.mca,invisible=c("ind","quali.sup"), cex=0.5)
```
En esta primera figura se representan las relaciones entre las modalidades de todas las variables categóricas con las dos primeras dimensiones del MCA. Se observa que la dimensión 1 se asocia con las variables que tienen relación con la edad, como la modalidad de Pensionista y Viudo. Se aprecia que la dimensión 2 se asocia a las modalidades relacionadas con la cualificación del trabajo del individuo, con una asociación positiva entre la dimensión y la cualificación del trabajador. Por tanto, se llamará a la dimensión 1 Edad, y a la dimensión 2 como cualificiación del trabajador. 


```{r, warning = FALSE, echo=F,fig.cap = "Variabilidad de las Variables Categóricas en las Dos Primeras Dimensiones", fig.show='hold',out.width="75%",out.height="75%"}
fviz_mca_var(res.mca, choice = "var", repel=TRUE)
```

En el gráfico obtenido puede verse la variabilidad que expresan cada una de las variables categóricas en función de las dimensiones 1 y 2. Aquellas variables que estén más cerca del origen de coordenadas aportan muy poca información respecto a la variabilidad de los datos y, por tanto, son poco importantes. En cambio, aquellas variables más alejadas del centro aportan información más relevante.


Se representan gráficamente la inercia que explica cada una de las dimensiones generadas:

```{r, echo=FALSE, include=FALSE}
library(ggplot2)

# Crear un data frame con los datos
data <- data.frame(Dimension = 1:nrow(res.mca$eig), 
                   Percentatge = res.mca$eig[, 2])

# Crear el gráfico de barras con ggplot2
inercia <- ggplot(data, aes(x = as.factor(Dimension), y = Percentatge)) +
  geom_bar(stat = "identity", fill = "#ADD8E6") +
  ylim(0, 25) +
  labs(x = "Dimensiones",
       y = "Porcentaje de varianza explicada",
       title = "Inercia explicada por cada dimensión") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
        panel.grid.major = element_blank())  # Elimina la cuadrícula de fondo


```

```{r, echo=F,fig.cap = "Inercia Explicada por cada Dimensión", fig.show='hold',out.width="75%",out.height="75%"}
inercia
```


Si una dimensión tiene una inercia baja, significa que todas las modalidades están muy cercanas al centro de gravedad y, en consecuencia, son muy similares. A medida que aumenta la inercia, va aumentando la distancia al centro de gravedad y, por tanto, se reduce la similitud.

Para poder estudiarlo más a fondo, se realiza la siguiente tabla en la que se puede observar para cada dimensión, su valor propio, el porcentaje de varianza (o inercia) explicada, y el porcentaje de varianza (o inercia) acumulada:

```{r,echo=F}
rounded_eigenvalues <- round(res.mca$eig, 2)

eigenvalue_table <- as.data.frame(rounded_eigenvalues)

# Mostrar la tabla
kbl(eigenvalue_table, col.names = c("Valor propio", "Porcentaje de la Varianza Acumulada", "Porcentaje de Varianza"),
      caption = "Varianza Explicada por cada Dimensión", booktabs = T) %>%
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

```
Tenemos un total de 31 dimensiones. La dimensión 1 destaca muy por encima del resto, explicando un 20.85% de la variabilidad de los datos, seguida de la dimensión 2, explicando un 9.45% de la variabilidad de los datos los datos. A partir de la dimensión 6, se ve que la gráfica se estabiliza bastante ya hasta la última dimensión.

Por tanto, en total las dos primeras dimensiones ya explican un 30.3% de la variabilidad de los datos, y se necesitan 17 dimensiones para llegar a tener una inercia acumulada por encima del 80%. 

Aunque las primeras dos dimensiones expliquen cerca del 30% de la inercia, no todos los puntos se muestran igual de bien en las dos dimensiones. La calidad de la representación se llama coseno cuadrado (cos2), que mide el grado de asociación entre categorías de variables y un eje particular.

A continuación, se representa la calidad de las categorias a partir de ajustar los colores para cada punto proyectado, tomando como criterio el valor del coseno cuadrático (cos2). Si una categoría de variable está bien representada por dos dimensiones, la suma de cos2 es cercana a uno. Para algunos de los elementos de la fila, se requieren más de dos dimensiones para representar perfectamente los datos. Se considera lo siguiente:

  - Las categorías de variables con valores bajos de cos2 se colorearán en "cian".
  - Las categorías de variables con valores medios de cos2 se colorearán en “amarillo”.
  - Las categorías de variables con valores altos de cos2 se colorearán en “rojo”.



````{r, warining = FALSE, echo=F,fig.cap = "Calidad de las Variables Categóricas a partir del Coseno Cuadrático", fig.show='hold',out.width="75%",out.height="75%" }
# Color by cos2 values: quality on the factor map
fviz_mca_var(res.mca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, # Avoid text overlapping
             ggtheme = theme_minimal())
````
Salen muchas categorías que no están muy bien representadas por las dos primeras dimensiones. Esto implica que la posición de los puntos correspondientes en el diagrama de dispersión debe interpretarse con cierta cautela. Probablemente sea necesaria una solución de mayor dimensión. Aún así, se ha decidido no realizar el MCA de mayores dimensiones debido a la dificultad de representación gráfica. Por tanto, los resultados del MCA se analizarán con cautela, especialmente las variables poco representadas en las dos primeras dimensiones.


## Gráfico de individuos y variables

Para una primera visualización de la relación entre las variables y las observaciones en un espacio reducido de dimensiones, acudimos al gráfico biplot. Este gráfico nos facilita la interpretación de la estructura de los datos al proporcionar una visualización que muestra la relación entre variables categóricas y observaciones. 
```{r, warining = FALSE, echo=F,fig.cap = "Biplot de Individuos y Categorías", fig.show='hold',out.width="75%",out.height="75%"}

 fviz_mca_biplot(res.mca, col.var = "#FC4E07", col.ind = "#00AFBB", ggtheme = theme_minimal())

```

Con tal de poder analizar estos resultados de manera más precisa, se decide dividir este gráfico y analizar por una parte únicamente el gráfico de observaciones y por otra parte el gráfico de las variables categóricas.


## Gráfico de individuos

Se representa gráficamente cómo se distribuyen los individuos en función de las dos primeras dimensiones que explican un 29.99% de la variabilidad:

```{r, warining = FALSE, echo=F,fig.cap = "Gráfico de Individuos en las dos Primeras Dimensiones", fig.show='hold',out.width="75%",out.height="75%"}
fviz_mca_ind(res.mca, geom = "point", col.ind = "black")
```
A simple vista, se aprecian varios grupos de individups pero resulta difícil distinguir cuántos. Sin embargo, sí podríamos decir que los individuos se dividen en como mínimo 2 grupos. Para distinguir mejor las agrupaciones de individuos y su asociación con algunas modalidades se pasa a estudiar cada variable para observar si existe algún tipo de asociación entre ellas.

### Gráfico de los individuos según variable TARGET

A continuación representamos los mismos individuos pero coloreandolos según la variable "target", es decir, nuestra varible output, donde 1 indica aquel cliente con dificultades de pago, y 0 contrariamente:

```{r, warining = FALSE, echo=F,fig.cap = "Gráfico de Individuos según la Variable TARGET en las dos Primeras Dimensiones", fig.show='hold',out.width="75%",out.height="75%"}
library(FactoMineR)

fviz_mca_ind(res.mca, geom = "point", col.ind = df_preprocessed$TARGET, 
             palette = c("orange", "purple")) 

```
Observamos como diferenciando a los individuos según si tienen dificultades o no con el pago, no hay diferencias entre grupos de individuos, por lo tanto, podemos decir que no se ve ninguna asociación entre la variable TARGET y las modalidades de las variables representadas en estas dos dimensiones. 


## Gráfico de variables

Para tener una representación aún más clara sobre las variables y su asociación, a continuación se grafican estas variables con curvas de densidad para ver aquellas zonas donde hay una mayor concentración.

```{r, echo=FALSE, include=FALSE}
res.mca$var$contrib
```

```{r, warining = FALSE, echo=F, fig.cap = "Representación de Variables en las Dimensiones del ACM", fig.show='hold',out.width="75%",out.height="75%"}
# Totes les variables actives
newbd<- dd[1:7]
res.mca1<-MCA(newbd, method="Burt", graph=FALSE)
cats = apply(newbd, 2, function(x) nlevels(as.factor(x)))

mca1_vars_df = data.frame(res.mca1$var$coord, Variable = rep(names(cats), cats))
mca1_obs_df = data.frame(res.mca1$ind$coord)

ggplot(data = mca1_vars_df, aes(x = Dim.1, y = Dim.2)) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_point(colour = "gray50", alpha = 0.7) +
  geom_density2d(colour = "gray80") +
  geom_text(data = mca1_vars_df, 
            aes(x = Dim.1, y = Dim.2, 
                label = rownames(mca1_vars_df), colour = Variable), cex = 3) +
  
  ggtitle("Gráfico MCA de variables") +
  scale_colour_discrete(name = "Variable")+ 
  xlim(-2,1.5) + 
  ylim(-1,1.5) 
```

Como bien se ha comentado, la primera dimensión está asociada con las variables que tienen relación con la edad y la segunda dimensión se asocia a modalidades con la cualificación del trabajo del individuo. 

Así pues, a partir de este gráfico de densidades, podemos  ver como hay una relación muy destacada entre Widow y Pensioner en la dimensión 1. Esta correlación puede deberse a  eventos de vida como la pérdida del cónyuge a una elevada edad y factores sociales, finalización laboral y por eso a una pensión por los años trabajados. La asociación de ambos términos con la población anciana y la edad es evidente.

En la dimensión 2 podemos ver una relación entre State Servant y Businessman con Mid/High Skill Laborers y con Education y Medicine. También podríamos ver relación con Higher education. La gente que trabaja en educación y/o mundo sanitario requieren un alto nivel de estudios y son trabajadores altamente cualificados. Al igual que podríamos asociarlo con los trabajadores en Servicios Públicos y Empresarios.

Aún y ver relación en este gráfico, hemos decidido realizar un gráfico de dispersión por cada una de las variables para ver si podíamos adquirir más información.


### Gráficos de dispersión agrupado por cada variable

Con el objetivo de ver si las categorías son significativamente diferentes entre sí, se grafican gráficos de elipses alrededor de las categorías de cada una de las variables.

Se considererá que las categorías con elipses no superpuestas, es decir, separadas entre sí, son significativament diferentes entre sí. Por el contrario, cuando las elipses se superponen, nos indican que hay una similitud o asociación entre categorías, es decir, no són significativamente diferentes entre ellas.

````{r, echo=FALSE, include=FALSE}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))
```

```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses NAME EDUCATION TYPE", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[1] 
```

```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses NAME FAMILY STATUS", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[2] 
```
```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses CODE GENDER", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[3] 
```
```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses NAME INCOME TYPE", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[4] 
```
```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses OCCUPATION TYPE", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[5] 
```
```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses ORGANITATION TYPE", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[6] 
```
```{r, warining = FALSE, echo=F, fig.cap = "Gráfico de Elipses REGION RATING CLIENT", fig.show='hold',out.width="75%",out.height="75%"}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[7] 
```

Al analizar cada variable de manerea individial en las dos primeras dimensiones del ACM, se observa que hay muchos gráficos donde encontramos categorías superpuestas entre ellas, y que por tanto, no nos aportan información significativa.


````{r, include=FALSE}
# for (k in 1:7){ #para cada variable
#   for(i in 1:5) {   # 1a dimension
#   for (j in 1:5) {  # 2a dimension
#     if (i != j) {
#       grafico <- plotellipses(res.mca,keepvar = "all", axes = c(i, j))[k]
#       plot(grafico)
#     }
#   }
# }  
# }
# 
````


Como se ha observado que en las dos primeras dimensiones únicamente se muestra aproximadamente el 30% de la variabilidad, para poder estudiar las categorías y su asociación más a fondo, hemos probado de analizar tres dimensions y no hi hay ningun gráfico significativo que nos permita extraer más información de la que ya hemos extraido con dos dimensiones. Por lo tanto, nos quedamos con los análisis sacados a partir de los gráficos de las dos primeras dimensiones. 


