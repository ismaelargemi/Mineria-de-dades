---
title: "Untitled"
author: "Iker Meneses Sales"
date: "2023-10-11"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## LDA (Linear Discriminant Analysis)

Para comenzar con los modelos discriminantes, se realizará en primer lugar un linear discriminant analysis (LDA) con el objetivo de intentar separar aquellos clientes que puedan tener dificultades de pago con aquellos solventes. Así pues, se procede a realizar dicho análisis discriminante.

```{r, include = F, warning=F, include=F}
library(MASS)
library(dplyr)
library(withr)
library(kableExtra)
library(FactoMineR)
library(factoextra)
library(gridExtra)
library(pROC)
library(doBy)
library(caret)
library(ROSE)
```

```{r, include = F}
load("C:/Users/iker1/Downloads/Dades preprocessades.RData")
```


```{r, warning = F, echo = F}
x = c("CNT_FAM_MEMBERS", "log_AMT_INCOME_TOTAL", "log_AMT_CREDIT", "AGE_YEARS", "RATIO_CREDIT_INCOME", "RATIO_ANNUITY_CREDIT", "DTI_RATIO")
y = "TARGET"

df_prep_num = df_preprocessed[,x]
df_prep_num = scale(df_prep_num)

df_prep_num = data.frame(df_prep_num, TARGET = df_preprocessed$TARGET)
```

Para ello, se recurrirá primero a un proceso de escalado de los datos a través de la función `scale()`, lo cual hará que todas las variables tengan un peso similar en la construcción del discriminante lineal. Una vez se ha realizado este proceso, el siguiente paso será realizar la partición de la base de datos disponible. Para ello, se realizará una partición clásica: el 80% de los datos se destinarán a entrenar el modelo y el otro 20, a validarlo. Además, dentro de la partición del train se realizará un proceso 5-fold validation con el objetivo de reducir el overfitting y proporcionar un modelo robusto.

```{r, echo = F, message = F, warning = F}
set.seed(123)
ind = createDataPartition(df_prep_num$TARGET, times=1, p=.8, list=FALSE)

train = df_prep_num[ind,]
validation = df_prep_num[-ind,]

# lda_cv = function(df_train, k){
# 
#   s = matrix(nrow = k, ncol = 7)
# 
#   for(i in 1:k){
#     ind = c(1+((i-1)*(nrow(df_train)/k)),(df_train/k)*i)
#     test = train[c(ind[1]:ind[2]),]
#     train_cv = train[-c(ind[1]:ind[2]),]
#     lda_cv = lda(reformulate(x,y), data = train_cv)
#   
#     s[i,] = lda_cv$scaling
#   }
# 
#   discriminante_res = colMeans(s)
# 
# }

ctrl = trainControl(method = "cv", number = 5)
lda = caret::train(reformulate(x,y), 
            data = train, 
            method = "lda", 
            trControl = ctrl, 
            metric = "Accuracy")

predicciones = factor(predict(lda, validation))

MC = confusionMatrix(validation$TARGET, predicciones)

tabla_lda = MC$table

rownames(tabla_lda) = c("No moroso","Potencial moroso")
colnames(tabla_lda) = c("No moroso","Potencial moroso")

kbl(tabla_lda,
    caption = "Matriz de confusión del conjunto de validación",
    booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position")) %>%
    add_header_above(c("","Realidad" = 2)) %>%
   pack_rows(index=c("Predicción"=2))
```

Apreciando los resultados obtenidos, se puede ver que la precisión obtenida por el modelo ha sido del `r round(MC$overall["Accuracy"]*100,4)`%, algo baja en comparación con ejemplos en otras áreas. Si desglosamos por sensibilidad y especificidad, vemos que los resultados en estos dos indicadores han sido de `r round(MC$byClass["Sensitivity"]*100,4)`% (mejor que en LDA), pero una especificidad del `r round(MC$byClass["Specificity"]*100,4)`%, respectivamente. Este hecho indica que los resultados están relativamente balanceados en cuanto a la hora de predecir se refiere. Sin embargo, a la hora de descubrir los valores predecidos positivos y los valores predecidos negativos, los resultados son de `r round(MC$byClass["Pos Pred Value"]*100,4)`% y `r round(MC$byClass["Neg Pred Value"]*100,4)`%. Este hecho nos dice que cuando el modelo predice que un cliente no es moroso, generalmente acierta pero, cuando el modelo predice que un cliente lo es, éste tiene problemas en la detección, resultando en problemas graves en la identificación de morosos. 


```{r, include = F, message = F, warning = F}
library(scales)
library(tidyverse)
library(ggplot2)
library(caret)

decisionplot <- function(model, x, cl = NULL, predict_type = "class",
                         resolution = 100) {
  if(!is.null(cl)) {
    x_data <- x %>% dplyr::select(-all_of(cl))
    cl <- x %>% pull(cl)
  } else cl <- 1
  k <- length(unique(cl))
  
  # resubstitution accuracy
  prediction <- predict(model, x_data, type = predict_type)
  if(is.list(prediction)) prediction <- prediction$class
  if(is.numeric(prediction))
    prediction <-  factor(prediction, labels = levels(cl))
  else
    prediction <- factor(prediction, levels = levels(cl))
  
  cm <- confusionMatrix(data = prediction, reference = cl)
  acc <- cm$overall["Accuracy"]
  
  # evaluate model on a grid
  r <- sapply(x[, 1:2], range, na.rm = TRUE)
  xs <- seq(r[1,1], r[2,1], length.out = resolution)
  ys <- seq(r[1,2], r[2,2], length.out = resolution)
  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))
  colnames(g) <- colnames(r)
  g <- as_tibble(g)
  
  ### guess how to get class labels from predict
  ### (unfortunately not very consistent between models)
  prediction <- predict(model, g, type = predict_type)
  if(is.list(prediction)) prediction <- prediction$class
  if(is.numeric(prediction))
    prediction <-  factor(prediction, labels = levels(cl))
  else
    prediction <- factor(prediction, levels = levels(cl))
  
  g <- g %>% add_column(prediction)
  
  ggplot(g, mapping = aes_string(
    x = colnames(g)[1],
    y = colnames(g)[2])) +
    geom_tile(mapping = aes(fill = prediction)) + 	scale_fill_brewer(palette="Pastel1")+
    geom_point(data = x, mapping =  aes_string(
      x = colnames(x)[1],
      y = colnames(x)[2],
      shape = colnames(x)[3], 
	  colour = colnames(x)[3]), alpha = 10, size = 4) +
	geom_point(data = x, mapping =  aes_string(
      x = colnames(x)[1],
      y = colnames(x)[2],
      shape = colnames(x)[3]), colour = "grey90", size = 1.5) + 
    labs(subtitle = paste("Training accuracy:", round(acc, 2)))
}

pca = fviz_pca_ind(prcomp(train[,x], scale = TRUE),  label = "none")

hola = pca$data[,2:3]
hola$pred = factor(predict(lda, train))
hola = as.tibble(hola)

lda = lda(reformulate(x,y), data = train)
decisionplot(lda, train, cl = "TARGET") + labs(title = "LDA")

```

Sin embargo, se sabe que el LDA puede presentar problemas en el momento en el que las variables no presentan normalidad o cuando las matrices de covarianzas son diferentes para cada grupo. Como ya se apreció en la descriptiva post-preprocessing, muchas de nuestras variables no presentaban normalidad, de forma que esto podría ser un problema de cara al uso del LDA. Es por eso por lo que se ha decidido realizar un QDA (Quadratic Discriminant Analysis) con el objetivo de corregir dichos problemas y mejorar la performance del LDA.


## QDA

Así pues, repitiendo el procedimiento seguido anteriormente en el LDA, toca repetir los mismos pasos para este modelo. De esta forma, los resultados obtenidos son los siguientes:

```{r, include = F, message = F, warning = F}
x = c("CNT_FAM_MEMBERS", "log_AMT_INCOME_TOTAL", "log_AMT_CREDIT", "AGE_YEARS", "RATIO_CREDIT_INCOME", "RATIO_ANNUITY_CREDIT", "DTI_RATIO")
y = "TARGET"

df_prep_num = df_preprocessed[,x]
df_prep_num = scale(df_prep_num)

df_prep_num = data.frame(df_prep_num, TARGET = df_preprocessed$TARGET)
```


```{r, message = F, warning = F, echo = F}
set.seed(123)
ind = createDataPartition(df_prep_num$TARGET, times=1, p=.8, list=FALSE)

train = df_prep_num[ind,]
validation = df_prep_num[-ind,]

# lda_cv = function(df_train, k){
# 
#   s = matrix(nrow = k, ncol = 7)
# 
#   for(i in 1:k){
#     ind = c(1+((i-1)*(nrow(df_train)/k)),(df_train/k)*i)
#     test = train[c(ind[1]:ind[2]),]
#     train_cv = train[-c(ind[1]:ind[2]),]
#     lda_cv = lda(reformulate(x,y), data = train_cv)
#   
#     s[i,] = lda_cv$scaling
#   }
# 
#   discriminante_res = colMeans(s)
# 
# }

ctrl = trainControl(method = "cv", number = 5)
qda = train(reformulate(x,y), 
            data = train, 
            method = "qda", 
            trControl = ctrl, 
            metric = "Accuracy")

predicciones = factor(predict(qda, validation))

MC = confusionMatrix(validation$TARGET, predicciones)

tabla_qda = MC$table

rownames(tabla_qda) = c("No moroso","Potencial moroso")
colnames(tabla_qda) = c("No moroso","Potencial moroso")

kbl(tabla_qda,
    caption = "Matriz de confusión del conjunto de validación",
    booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position")) %>%
    add_header_above(c("","Realidad" = 2)) %>%
   pack_rows(index=c("Predicción"=2))
```

Como se puede apreciar, los resultados obtenidos son bastante similares a los presentados en el discriminante lineal. De hecho, en este caso, la precisión ha sido del `r round(MC$overall["Accuracy"]*100,4)`%, algo peor que la del LDA. Si observamos sensibilidad y especificidad, apreciaremos que se ha obtenido una sensibilidad del `r round(MC$byClass["Sensitivity"]*100,4)`% (mejor que en LDA), pero una especificidad del `r round(MC$byClass["Specificity"]*100,4)`% (muchísimo peor comparado con el resultado del LDA). Si observamos otras métricas disponibles, apreciaremos nuevamente valores altos en la tasa de valores positivos predecidos (`r round(MC$byClass["Pos Pred Value"]*100,4)`%) y valores bajos en la tasa de valores negativos predecidos (`r round(MC$byClass["Neg Pred Value"]*100,4)`%). Sin embargo, la diferencia entre estos no es tan extrema como en LDA. Por último, podemos apreciar que el valor del F-score es de `r 2/((1/MC$overall["Accuracy"]) + (1/MC$byClass["Sensitivity"]))`.


Tras haber estudiado los resultados, se ha concluido que, si bien es cierto que los resultados obtenidos son mejorables, los datos presentan un ligero desbalanceo. Este hecho hace que las esimaciones proporcionadas puedan no ser del todo fiables, ya que es posible que los algoritmos tengan problemas en la detección de la clase minoritaria. Para ello, realizaremos un procedimiento undersampling con el objetivo de balancear nuestros datos evitando una mala calibración de los algoritmos. Así pues, repetimos el mismo proceso que el realizado anteriormente con los datos ahora balanceados.


## LDA (usando datos balanceados con undersampling)

En primer lugar, estandarizamos los datos usando la función de R `scale()` para así hacer que todos las variables tengan el mismo peso. Una vez los datos han sido normalizados y tienen todas las variables el mismo peso en el modelo, aplicamos ROSE:

```{r, warning = F, echo = F}
x = c("CNT_FAM_MEMBERS", "log_AMT_INCOME_TOTAL", "log_AMT_CREDIT", "AGE_YEARS", "RATIO_CREDIT_INCOME", "RATIO_ANNUITY_CREDIT", "DTI_RATIO")
y = "TARGET"

df_prep_num = df_preprocessed[,x]
df_prep_num = scale(df_prep_num)
```

```{r, include =F, warning=F, message=F}
library(unbalanced)

pred_vars = df_prep_num[,x]
response_vars = df_preprocessed[,y]
undersampled_data <- ubBalance(pred_vars, 
                               response_vars, 
                               type='ubUnder',         # Option for undersampling
                               verbose = F)
data_x = undersampled_data$X
data_balanced = cbind(data_x, factor(undersampled_data$Y))
data_balanced = data.frame(data_balanced)

names(data_balanced) = c(x, "TARGET")
data_balanced$TARGET = factor(data_balanced$TARGET)
```

Seguidamente, se realiza una partición del dataset entre train y validation con el objetivo de conseguir aproximaciones correctas que puedan ser usadas, evitando el over-fitting. Para ello, se realizará una partición 80-20 de la base de datos. Además, se realizará un 5-fold validation dentro del conjunto train, eliminando así cualquier problema de overfitting que pudiera existir:

```{r, echo = F}
set.seed(123)
ind = createDataPartition(data_balanced$TARGET, times=1, p=.8, list=FALSE)

train = data_balanced[ind,]
validation = data_balanced[-ind,]

# lda_cv = function(df_train, k){
# 
#   s = matrix(nrow = k, ncol = 7)
# 
#   for(i in 1:k){
#     ind = c(1+((i-1)*(nrow(df_train)/k)),(df_train/k)*i)
#     test = train[c(ind[1]:ind[2]),]
#     train_cv = train[-c(ind[1]:ind[2]),]
#     lda_cv = lda(reformulate(x,y), data = train_cv)
#   
#     s[i,] = lda_cv$scaling
#   }
# 
#   discriminante_res = colMeans(s)
# 
# }

ctrl = trainControl(method = "cv", number = 5)
lda = caret::train(reformulate(x,y), 
            data = train, 
            method = "lda", 
            trControl = ctrl, 
            metric = "Accuracy")

predicciones = factor(predict(lda, validation))

MC = confusionMatrix(validation$TARGET, predicciones)

tabla_lda = MC$table

rownames(tabla_lda) = c("No moroso","Potencial moroso")
colnames(tabla_lda) = c("No moroso","Potencial moroso")

kbl(tabla_lda,
    caption = "Matriz de confusión del conjunto de validación",
    booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position")) %>%
    add_header_above(c("","Realidad" = 2)) %>%
   pack_rows(index=c("Predicción"=2))
```


Esta vez, los resultados son más consistentes: los resultados obtenidos son similares a cuando los datos estaban desbalanceados, pero hay cambios en cuanto a los valores positivos y negativos se refiere. En este caso, la precisión obtenida ha sido del `r round(MC$overall["Accuracy"]*100,4)`%, dividido en una sensibilidad del `r round(MC$byClass["Sensitivity"]*100,4)`% y una especificidad del `r round(MC$byClass["Specificity"]*100,4)`%. Esta vez, se puede comprobar que los valores proporcionados por los datos de valor predecido positivo y valor predecido negativo están más balanceados, de forma que estos resultados parecen mucho más fiables.


Sin embargo, se sabe que el LDA puede presentar problemas en el momento en el que las variables no presentan normalidad o cuando las matrices de covarianzas son diferentes para cada grupo. Como ya se apreció en la descriptiva post-preprocessing, muchas de nuestras variables no presentaban normalidad, de forma que esto podría ser un problema de cara al uso del LDA. Es por eso por lo que se ha decidido realizar un QDA (Quadratic Discriminant Analysis) con el objetivo de corregir dichos problemas y mejorar la performance del LDA.

## QDA (usando datos balanceados con undersampling)

Así pues, repitiendo el procedimiento seguido anteriormente en el LDA, toca repetir los mismos pasos para este modelo. De esta forma, los resultados obtenidos son los siguientes:


```{r, echo = F}

# lda_cv = function(df_train, k){
# 
#   s = matrix(nrow = k, ncol = 7)
# 
#   for(i in 1:k){
#     ind = c(1+((i-1)*(nrow(df_train)/k)),(df_train/k)*i)
#     test = train[c(ind[1]:ind[2]),]
#     train_cv = train[-c(ind[1]:ind[2]),]
#     lda_cv = lda(reformulate(x,y), data = train_cv)
#   
#     s[i,] = lda_cv$scaling
#   }
# 
#   discriminante_res = colMeans(s)
# 
# }

ctrl = trainControl(method = "cv", number = 5)
qda = caret::train(reformulate(x,y), 
            data = train, 
            method = "lda", 
            trControl = ctrl, 
            metric = "Accuracy")

predicciones = factor(predict(qda, validation))

MC = confusionMatrix(validation$TARGET, predicciones)

tabla_qda = MC$table

rownames(tabla_qda) = c("No moroso","Potencial moroso")
colnames(tabla_qda) = c("No moroso","Potencial moroso")

kbl(tabla_qda,
    caption = "Matriz de confusión del conjunto de validación",
    booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position")) %>%
    add_header_above(c("","Realidad" = 2)) %>%
   pack_rows(index=c("Predicción"=2))
```

Por último, el modelo del QDA con datos balanceados presenta resultados similares al LDA. La precisión alcanzada por este modelo ha sido del `r round(MC$overall["Accuracy"]*100,4)`%, lo que retorna una sensibilidad del `r round(MC$byClass["Sensitivity"]*100,4)`% y una especificidad del `r round(MC$byClass["Specificity"]*100,4)`%. Este modelo corrige los problemas de desbalanceo que existían previamente, ya que ahora se puede apreciar cómo los resultados del modelo son más consistentes. Concretamente, el valor predictivo positivo y el valor predictivo negativo para este modelo son de `r round(MC$byClass["Pos Pred Value"]*100,4)`% `r round(MC$byClass["Neg Pred Value"]*100,4)`%, respectivamente. Para acabar, el F-score de este modelo es de `r 2/((1/MC$overall["Accuracy"]) + (1/MC$byClass["Sensitivity"]))`, algo más bajo que en LDA.

En resumen, observando los resultados obtenidos, se puede afirmar que los dos modelos discriminantes presentan resultados muy pobres: es probable que el hecho de añadir posteriormente las variables categóricas acabe de hacer que se mejore de forma clara los resultados conseguidos hasta ahora.

