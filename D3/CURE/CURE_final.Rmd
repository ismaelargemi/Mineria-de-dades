---
title: "Algoritmo CURE"
author: "Iker Meneses Sales"
date: "2023-10-29"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Algoritmo CURE

Siguiendo con los algoritmos de clusterización para bases de datos grandes, es momento de realizar el CURE. CURE (Clustering Using REpresentatives) es un algoritmo de clustering para base de datos grandes en el cual se gestiona, inicialmente, una muestra de la base de datos a partir de la cual se realiza un clustering jerárquico (usando la distancia euclídea y el método de agregación simple) y se sacan un número pequeño de puntos (representantes) de cada cluster. Entonces, se  acercan esos representantes hacia el centroide del cluster un 20% y, a partir de estos representantes acercados, se busca cuál es el que se encuentra más cercano de cada punto de la base de datos restante. Finalmente, una vez se encuentra el representante más cercano a cada individuo, se asigna el individuo al cluster al que pertenece el representante.

En este caso, como la base de datos escogida dispone de datos numéricos y categóricos, se ha decidido modificar las reglas del CURE y usar la distancia de Gower y el método de agregación de Ward en la construcción inicial del clustering. Así pues, realmente se podría afirmar que se está realizando un pseudoCURE en este caso.

Inicialmente, para este caso, se ha decidido escoger una muestra significativa y grande para evitar problemas en la construcción de los clusters iniciales. Así, se ha usado una muestra de $n=2000$ con el objetivo de realizar el primer cluster a partir del cual se elegirán los representantes. El dendograma resultante reporta la siguiente estructura:

```{r, include = F, warning = F, message = F}
library(dplyr)
library(cluster)
library(factoextra)
library(FactoMineR)
library(DescTools)
library(StatMatch)
library(kableExtra)
library(dendextend)
```

```{r, include=F}
load("C:/Users/iker1/Downloads/Dades preprocessades.RData")
data = df_preprocessed[!names(df_preprocessed) %in% c("AMT_INCOME_TOTAL", "AMT_CREDIT", "AMT_ANNUITY", "AMT_GOODS_PRICE", "DAYS_BIRTH", "log_AMT_GOODS_PRICE", "log_AMT_ANNUITY", "DIFF_CREDIT_GOODS")]
data_num = select_if(data, is.numeric)
data_factor = select_if(data, is.factor)
data_num = scale(data_num)
data_scaled = data.frame(data_num, data_factor)

data_scaled = data_scaled[!names(data_scaled) %in% c("TARGET")]
```

```{r, echo = F, warning = F, message = F}
set.seed(123)
ind = sample(1:5000, 2000, replace = F)
df_sample = data_scaled[ind,]
df_remaining = data_scaled[-ind,]

dist = daisy(df_sample, metric = "gower") 
clust = hclust(dist, method = "ward.D2")


a <- as.dendrogram(clust)
d1 <- color_branches(a,k=3,col=c('#458B74','#0000CD','#CD3333'),labels = F)
plot(d1, hang = -1, ylab = 'Altura', labels = F)
abline(h=4.5, col = 'red',lty = 'dashed')
```

Tras analizar los resultados, se puede apreciar que el número de clusters óptimo es $k=3$. De esta forma, la partición inicial de la muestra en cada cluster se puede apreciar en la parte inferior:

```{r, echo = F}
opt_clusters = 3
clustering = cutree(clust, opt_clusters)

a = table(clustering)
b = matrix(c(1:3, as.vector(a)), ncol = 2)
colnames(b) = c("Cluster", "Observaciones")

kbl(b, 
    caption = "Distribución inicial de individuos por cluster CURE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Ahora, a partir de este clustering jerárquico inicial, se escogerán los representantes. Para ello, se busca aquellos puntos más alejados entre sí y, a la vez, más alejados del centroide de cada cluster. Para este paso, se han elegido exactamente 5 representantes por cluster. Una vez se tienen seleccionados, el siguiente paso es acercarlos al centro. En este caso, se ha decidido aproximarlos un 20% hacia el centroide del cluster al que pertenecen.

```{r, include = F}
# Buscamos los centroides de cada cluster

grupo1 = df_sample[clustering == 1,]
grupo2 = df_sample[clustering == 2,]
grupo3 = df_sample[clustering == 3,]


# Encontramos centros de cada variables

aux = grupo1

filas = (nrow(aux)+1)

for(i in 1:ncol(grupo1)){
  if(is.numeric(grupo1[,i])){
    aux[filas,i] = mean(grupo1[,i])
  }else{
    aux[filas,i] = names(sort(table(grupo1[,i]), decreasing = TRUE)[1])
  }
}

for(i in 1:ncol(aux)){
  if(is.numeric(grupo2[,i])){
    aux[filas+1,i] = mean(grupo2[,i])
  }else{
    aux[filas+1,i] = names(sort(table(grupo2[,i]), decreasing = TRUE)[1])
  }
}

for(i in 1:ncol(aux)){
  if(is.numeric(grupo3[,i])){
    aux[filas+2,i] = mean(grupo3[,i])
  }else{
    aux[filas+2,i] = names(sort(table(grupo3[,i]), decreasing = TRUE)[1])
  }
}

# En "centroides" tenemos los dos centroides de los grupos encontrados

centroides = aux[(nrow(aux)-2):nrow(aux),]

# Ahora, encontramos a nuestros representantes

m2 <- function(datos, n_repres){
  
  subset <- datos
  
  alldist <- as.matrix(daisy(datos, metric = "gower"))
  
  while (nrow(subset) > n_repres) {
    cdists = rowSums(alldist)
    closest <- which(cdists == min(cdists))[1]
    subset <- subset[-closest,]
    alldist <- alldist[-closest,-closest]
  }
  return(subset)
}

repres_1 = m2(grupo1, 5)
repres_2 = m2(grupo2, 5)
repres_3 = m2(grupo3, 5)


# Los acercamos al centro (un 20%):

cols_num = which(sapply(repres_1, is.numeric))
repres_proximos1 = repres_1

for(i in 1:nrow(repres_1)){
  repres_proximos1[i,cols_num] = ((repres_1[i,cols_num] - centroides[1,cols_num])*0.8) + centroides[1,cols_num]
}

cols_num = which(sapply(repres_2, is.numeric))
repres_proximos2 = repres_2

for(i in 1:nrow(repres_2)){
  repres_proximos2[i,cols_num] = ((repres_2[i,cols_num] - centroides[2,cols_num])*0.8) + centroides[2,cols_num]
}

cols_num = which(sapply(repres_3, is.numeric))
repres_proximos3 = repres_3

for(i in 1:nrow(repres_3)){
  repres_proximos3[i,cols_num] = ((repres_3[i,cols_num] - centroides[3,cols_num])*0.8) + centroides[3,cols_num]
}

# Unimos los representantes:

repres_finales = bind_rows(repres_proximos1, repres_proximos2, repres_proximos3)
```

Por último, se analiza cada punto y se busca el representante más cercano. Una vez se tiene esa información, se le asigna al individuo el cluster al que pertenece el representante más cercano. Para este paso, se ha procedido a procesar los datos de 500 en 500, para así evitar problemas con la capacidad de gestión de datos del ordenador. Así, el resultado del clustering final se presenta en la taba inferior:

```{r, echo = F}
# Ahora, vamos cogiendo grupos de n=500 hasta acabar la muestra:

restantes = df_remaining

conj1 = grupo1
conj2 = grupo2
conj3 = grupo3

for(i in 1:6){
  ind = sample(1:(3000-500*(i-1)), 500)
  submuestra = restantes[ind,]
  restantes = restantes[-ind,]
  
  df_sample = bind_rows(df_sample, submuestra)
  df_recorrido = data.frame(df_sample)
  
  distancia_vs_representantes = StatMatch::gower.dist(submuestra, repres_finales)
  
  # Por último, asignamos la observación al cluster del representante más cercano
  
  indices_minimos = apply(distancia_vs_representantes, 1, function(row) which.min(row))
  
  for(k in 1:500){
    if(indices_minimos[k]<6){
      indices_minimos[k] = 1
    }else{
      if(indices_minimos[k]<11){
        indices_minimos[k] = 2
      }else{
        indices_minimos[k] = 3
      }
    }
  }
  
  conj1 = bind_rows(conj1, submuestra[indices_minimos == 1,])
  conj2 = bind_rows(conj2, submuestra[indices_minimos == 2,])
  conj3 = bind_rows(conj3, submuestra[indices_minimos == 3,])
}

# Así pues, las variables conj1 y conj2 conforman los clusters resultantes finales


b = matrix(c(1:3, nrow(conj1), nrow(conj2), nrow(conj3)), nrow = 3)


kbl(b, 
    caption = "Distribución inicial de individuos por cluster CURE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

