---
output: pdf_document
header-includes:
  \usepackage{fullpage} 
  \usepackage[spanish]{babel}
  \setlength{\headsep}{7mm} 
  \usepackage[linktoc=page]{hyperref}
  \usepackage{fancyhdr}
  \usepackage{floatrow}
  \floatsetup[figure]{capposition=top}
  \floatsetup[table]{style=plaintop}
  \usepackage{float}
  \floatplacement{figure}{H}
  \newcommand{\beginsupplement}{
  \setcounter{table}{36}  
  \renewcommand{\thetable}{\arabic{table}} 
  \setcounter{figure}{110} 
  \renewcommand{\thefigure}{\arabic{figure}}}
  \setlength{\headheight}{13.6pt}
  \setlength{\topmargin}{-10mm}
  \rhead{Minería de Datos}
  \lhead{Entrega D3}
---

```{=tex}
\setlength{\headheight}{13.6pt}
\setlength{\topmargin}{-10mm}
```

\pagestyle{fancy}

```{=tex}
\rhead{Mineria de Datos}
\lhead{Entrega D3}
```


```{=tex}
\cfoot{\thepage}
\setcounter{page}{121}
\setcounter{figure}{110}
\setcounter{table}{36}
```

## Algoritmo CURE

Siguiendo con los algoritmos de clusterización para bases de datos grandes, es momento de realizar el CURE. CURE (Clustering Using REpresentatives) es un algoritmo de clustering para base de datos grandes en el cual se gestiona, inicialmente, una muestra de la base de datos a partir de la cual se realiza un clustering jerárquico (usando la distancia euclídea y el método de agregación simple) y se sacan un número pequeño de puntos (representantes) de cada cluster. Entonces, se  acercan esos representantes hacia el centroide del cluster un 20% y, a partir de estos representantes acercados, se busca cuál es el que se encuentra más cercano de cada punto de la base de datos restante. Finalmente, una vez se encuentra el representante más cercano a cada individuo, se asigna el individuo al cluster al que pertenece el representante.

En este caso, como la base de datos escogida dispone de datos numéricos y categóricos, se ha decidido modificar las reglas del CURE y usar la distancia de Gower y el método de agregación de Ward en la construcción inicial del clustering. Así pues, realmente se podría afirmar que se está realizando un pseudoCURE en este caso.

Inicialmente, para este caso, se ha decidido escoger una muestra significativa y grande para evitar problemas en la construcción de los clusters iniciales. Así, se ha usado una muestra de $n=2000$ con el objetivo de realizar el primer cluster a partir del cual se elegirán los representantes. El dendograma resultante reporta la siguiente estructura:

```{r, include = F, warning = F, message = F}
library(dplyr)
library(cluster)
library(factoextra)
library(FactoMineR)
library(DescTools)
library(StatMatch)
library(kableExtra)
library(dendextend)
```

```{r, include=F}
load("C:/Users/iker1/Downloads/Dades preprocessades.RData")
data = df_preprocessed[!names(df_preprocessed) %in% c("AMT_INCOME_TOTAL", "AMT_CREDIT", "AMT_ANNUITY", "AMT_GOODS_PRICE", "DAYS_BIRTH", "log_AMT_GOODS_PRICE", "log_AMT_ANNUITY", "DIFF_CREDIT_GOODS")]
dades = data
data_num = select_if(data, is.numeric)
data_factor = select_if(data, is.factor)
data_num = scale(data_num)
data_scaled = data.frame(data_num, data_factor)

data_scaled = data_scaled[!names(data_scaled) %in% c("TARGET")]
data_scaled['index'] = 1:5000
clusterIndividuo <- vector(mode='list', length=5000)
```

```{r, echo = F, warning = F, message = F}
set.seed(123)
ind = sample(1:5000, 2000, replace = F)
df_sample = data_scaled[ind,]
df_remaining = data_scaled[-ind,]
indRemaining = 1:5000
indRemaining = indRemaining[-ind]

dist = daisy(df_sample[!names(data_scaled) %in% c("index")], metric = "gower") 
clust = hclust(dist, method = "ward.D2")


a <- as.dendrogram(clust)
d1 <- color_branches(a,k=3,col=c('#458B74','#0000CD','#CD3333'),labels = F)

```

```{r, echo=F,fig.cap = "Dendograma inicial CURE", fig.show='hold',out.width="75%",out.height="75%"}
plot(d1, ylab = 'Altura')
abline(h=4.5, col = 'red',lty = 'dashed')
```


Tras analizar los resultados, se puede apreciar que el número de clusters óptimo es $k=3$. De esta forma, la partición inicial de la muestra en cada cluster se puede apreciar en la parte inferior:

```{r, echo = F}
opt_clusters = 3
clustering = cutree(clust, opt_clusters)
for (i in 1:2000) {
  clusterIndividuo[ind[i]] = clustering[i]
}
a = table(clustering)
b = matrix(c(1:3, as.vector(a)), ncol = 2)
colnames(b) = c("Cluster", "Observaciones")

kbl(b, 
    caption = "Distribución inicial de individuos por cluster CURE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Ahora, a partir de este clustering jerárquico inicial, se escogerán los representantes. Para ello, se busca aquellos puntos más alejados entre sí y, a la vez, más alejados del centroide de cada cluster. Para este paso, se han elegido exactamente 5 representantes por cluster. Una vez se tienen seleccionados, el siguiente paso es acercarlos al centro. En este caso, se ha decidido aproximarlos un 20% hacia el centroide del cluster al que pertenecen.

```{r, include = F}
# Buscamos los centroides de cada cluster

grupo1 = df_sample[clustering == 1,]
grupo2 = df_sample[clustering == 2,]
grupo3 = df_sample[clustering == 3,]


# Encontramos centros de cada variables

aux = grupo1

filas = (nrow(aux)+1)

for(i in 1:ncol(grupo1)){
  if(is.numeric(grupo1[,i])){
    aux[filas,i] = mean(grupo1[,i])
  }else{
    aux[filas,i] = names(sort(table(grupo1[,i]), decreasing = TRUE)[1])
  }
}

for(i in 1:ncol(aux)){
  if(is.numeric(grupo2[,i])){
    aux[filas+1,i] = mean(grupo2[,i])
  }else{
    aux[filas+1,i] = names(sort(table(grupo2[,i]), decreasing = TRUE)[1])
  }
}

for(i in 1:ncol(aux)){
  if(is.numeric(grupo3[,i])){
    aux[filas+2,i] = mean(grupo3[,i])
  }else{
    aux[filas+2,i] = names(sort(table(grupo3[,i]), decreasing = TRUE)[1])
  }
}

# En "centroides" tenemos los dos centroides de los grupos encontrados

centroides = aux[(nrow(aux)-2):nrow(aux),]

# Ahora, encontramos a nuestros representantes

m2 <- function(datos, n_repres){
  
  subset <- datos
  
  alldist <- as.matrix(daisy(datos, metric = "gower"))
  
  while (nrow(subset) > n_repres) {
    cdists = rowSums(alldist)
    closest <- which(cdists == min(cdists))[1]
    subset <- subset[-closest,]
    alldist <- alldist[-closest,-closest]
  }
  return(subset)
}

repres_1 = m2(grupo1, 5)
repres_2 = m2(grupo2, 5)
repres_3 = m2(grupo3, 5)


# Los acercamos al centro (un 20%):

cols_num = which(sapply(repres_1, is.numeric))
repres_proximos1 = repres_1

for(i in 1:nrow(repres_1)){
  repres_proximos1[i,cols_num] = ((repres_1[i,cols_num] - centroides[1,cols_num])*0.8) + centroides[1,cols_num]
}

cols_num = which(sapply(repres_2, is.numeric))
repres_proximos2 = repres_2

for(i in 1:nrow(repres_2)){
  repres_proximos2[i,cols_num] = ((repres_2[i,cols_num] - centroides[2,cols_num])*0.8) + centroides[2,cols_num]
}

cols_num = which(sapply(repres_3, is.numeric))
repres_proximos3 = repres_3

for(i in 1:nrow(repres_3)){
  repres_proximos3[i,cols_num] = ((repres_3[i,cols_num] - centroides[3,cols_num])*0.8) + centroides[3,cols_num]
}

# Unimos los representantes:

repres_finales = bind_rows(repres_proximos1, repres_proximos2, repres_proximos3)
```

Por último, se analiza cada punto y se busca el representante más cercano. Una vez se tiene esa información, se le asigna al individuo el cluster al que pertenece el representante más cercano. Para este paso, se ha procedido a procesar los datos de 500 en 500, para así evitar problemas con la capacidad de gestión de datos del ordenador. Así, el resultado del clustering final se presenta en la taba inferior:

```{r, echo = F}
# Ahora, vamos cogiendo grupos de n=500 hasta acabar la muestra:

restantes = df_remaining

conj1 = grupo1
conj2 = grupo2
conj3 = grupo3


for(i in 1:6){
  lb = 1+(i-1)*500; ub = 500*i
  indSubmuestra = indRemaining[lb:ub]
  submuestra = data_scaled[indSubmuestra,]
  
  distancia_vs_representantes = StatMatch::gower.dist(submuestra[!names(data_scaled) %in% c("index")], repres_finales[!names(data_scaled) %in% c("index")])
  
  # Por último, asignamos la observación al cluster del representante más cercano
  
  indices_minimos = apply(distancia_vs_representantes, 1, function(row) which.min(row))
  
  for(k in 1:500){
    if(indices_minimos[k]<6){
      indices_minimos[k] = 1
      clusterIndividuo[indSubmuestra[k]] = 1
    }else{
      if(indices_minimos[k]<11){
        indices_minimos[k] = 2
        clusterIndividuo[indSubmuestra[k]] = 2
      }else{
        indices_minimos[k] = 3
        clusterIndividuo[indSubmuestra[k]] = 3
      }
    }
  }
  
  conj1 = bind_rows(conj1, submuestra[indices_minimos == 1,])
  conj2 = bind_rows(conj2, submuestra[indices_minimos == 2,])
  conj3 = bind_rows(conj3, submuestra[indices_minimos == 3,])
}

# Así pues, las variables conj1 y conj2 conforman los clusters resultantes finales


b = matrix(c(1:3, nrow(conj1), nrow(conj2), nrow(conj3)), nrow = 3)


kbl(b, 
    caption = "Distribución Final de individuos por cluster CURE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

cluster = clusterIndividuo
```

## Profiling del CURE

A partir del CURE resultante, se analizan las características que diferencia cada cluster encontrado para así identificar las diferencias significativas más relevantes entre grupos. Aquí se muestran los gráficos de las medias por grupo para cada cluster resultante:


```{r, include=F}
individuos_clusters = bind_rows(conj1, conj2, conj3)

individuos_clusters$cluster_cure = c(rep(1,nrow(conj1)), rep(2,nrow(conj2)), rep(3,nrow(conj3)))
individuos_clusters$cluster_cure = factor(individuos_clusters$cluster_cure)

datos_originales = individuos_clusters
  
nums = select_if(individuos_clusters, is.numeric)
means_original = colMeans(select_if(data, is.numeric))
sd_original = sapply(select_if(data, is.numeric), sd)

datos_no_stand = matrix(ncol = 8, nrow = 5000)
for(i in 1:nrow(nums)){
  for(j in 1:(ncol(nums)-1)){
    datos_no_stand[i,j] = (nums[i,j]*sd_original[j]) + means_original[j]
  }
}

datos_no_stand = cbind(datos_no_stand, nums[,9])

colnames(datos_no_stand) = colnames(nums)
datos_no_stand = data.frame(datos_no_stand)

datos_no_stand = data.frame(datos_no_stand, select_if(individuos_clusters, is.factor))
datos_no_stand$TARGET = data$TARGET[datos_no_stand$index]
```



```{r, include = F}
dades <- datos_no_stand[,!(names(datos_no_stand) %in% c('cluster_cure', 'index'))]
cluster <- individuos_clusters$cluster_cure
```

Antes de empezar a analizar cada variable, es importante destacar qué variables son sigificativas para diferenciar clusters. Para ello, se realizará un test de Chi-cuadrado para las variables categóricas y, por otro lado, un test F para las variables numéricas, a través de una tabla ANOVA:

**Significación de las variables categóricas**

En la siguiente tabla se puede apreciar cada variable con su p-valor asociado a la prueba de Chi-cuadrado correspondiente:

```{r, echo = F, warning= F}
indices_categoricas <- sapply(dades, is.factor)
p_values <- numeric(length(dades))

# Realiza pruebas de chi-cuadrado para las variables categóricas
for (i in which(indices_categoricas)) {
  variable_categorica <- dades[, i]
  cross_table <- table(variable_categorica, cluster)
  chi_square_result <- chisq.test(cross_table)
  p_values[i] <- chi_square_result$p.value
}


# Muestra los resultados
p_values_categoricas <- data.frame(
  Variable = names(dades)[indices_categoricas],
  P_Value = p_values[indices_categoricas]
)

options(scippen=000)

kbl(p_values_categoricas, 
    caption = "P-valor asociado a cada variable categórica", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Como se puede apreciar, en este caso, todas las variables son significativas, es decir, existen diferencias entre al menos un par de clusters. De esta forma, todas las variables categóricas serán tenidas en cuenta para el perfilamiento de los clusters.

**Significación de las variables numéricas**

Seguidamente, se seguirá el mismo procedimiento para las variables numéricas. Esta vez, sin embargo, se usarán los test F resultantes de la tabla ANOVA:


```{r, echo = F, warning = F, message = F}
library(car)
# Identifica las variables categóricas (factores)
variables_categoricas <- sapply(dades, is.factor)

# Filtra las variables numéricas
variables_numericas <- dades[!variables_categoricas]
# Inicializa un objeto para almacenar los p valores de la ANOVA
p_values_numericas <- list()

# Realiza un ANOVA para cada variable numérica en función de cluster6 y cluster3
for (i in 1:ncol(variables_numericas)) {
  variable_name <- names(variables_numericas)[i]
  
  formula_cluster <- as.formula(paste(variable_name, " ~ cluster"))
  anova_result_cluster <- Anova(lm(formula_cluster, data = variables_numericas))
  
  # Almacena los p-valores de la ANOVA en p_values_anova
  p_values_numericas[[variable_name]] <- c(Cluster = anova_result_cluster[["Pr(>F)"]][1])
}

p_values_numericas = unlist(p_values_numericas)
pv = matrix(p_values_numericas)
rownames(pv) = names(select_if(dades, is.numeric))

# Muestra el objeto con los p-valores de la ANOVA
kbl(pv, 
    caption = "P-valor asociado a cada variable numérica", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Nuevamente, como se puede apreciar, todas las variables son significativas, es decir, existen diferencias entre al menos un par de clusters. De esta forma, todas las variables categóricas serán tenidas en cuenta para el perfilamiento de los clusters.

```{r, echo = F}
# Función para evaluar significancia de p-valores
evaluate_p_values <- function(p_values, alpha = 0.05) {
  significance <- ifelse(p_values <= alpha, 1, 0)
  return(significance)
}

# Evalúa significancia de p-valores para variables numéricas y categóricas
significance_numericas <- evaluate_p_values(p_values_numericas)
significance_categoricas <- evaluate_p_values(p_values_categoricas[,2])

# print("Significancia de p-valores para variables numéricas:")
# print(significance_numericas)

names(significance_categoricas) <- p_values_categoricas[1,]
# print("Significancia de p-valores para variables categóricas:")
# print(significance_categoricas)
```

### Análisis del profiling

Una vez ya hemos presenciado que todas las variables se usarán en el proceso de profiling de cada cluster, se ha procedido a realizar gráficos para cada variable, para así ver las características de cada grupo. Todos los gráficos realizados para el análisis de cada cluster se hallan enn los anexos finales del trabajo. Así pues, tras haber analizado atentamente el resultado ofrecido por el profiling realizado, se han extraído las siguientes conclusiones para cada cluster:


-   Cluster 1: Este cluster está formado por la gente con un ratio credit/income menor, además de un DTI ratio más bajo que los otros dos clusters encontrados. Esto nos indica que son personas más responsables con sus cuentas y que piden crédito cuando su situación financiera es positiva, ya que tienden a endeudarse menos. Además, apreciando el análisis de las variables categóricas, se aprecia que este cluster presenta una mayor concentración de hombres (solteros en su mayoría) y dedicados principalmente al sector de la banca, en su mayoría como comerciales de este mismo sector. De esta forma, es lógico pensar que hagan una buena gestión de sus finanzas personales.

-   Cluster 2: Este grupo se caracteriza principalmente por tener un ratio credit/income más alto y un ratio annuity/credit menor. Es decir, es gente que pide préstamos por una cantidad elevada en relación a sus ingresos, pero que generalmente deciden pagarlo a largo plazo. Est hehco, además, va relacionado con que la edad del coche media sea la menor entre los tres grupos: tal vez una parte del préstamo solicitado se ha destinado al coche. Entrando en el análisis de las variables categóricas, se aprecia que en su mayoría son mujeres que ocupan trabajos de gran capital humano (state servant) y residen en lugares con un buen rating por la empresa.

-   Cluster 3: Por último, en este cluster se sitúan aquellos ciudadanos con una edad superior, en su gran mayoría pensionistas. Además, poseen coches con más años que el resto y presentan núcleos familiares más reducidos. En este cluster se encuentran la gran mayoría de personas viudas y, en general, el nivel educativo que más predomina es la secundaria. 
