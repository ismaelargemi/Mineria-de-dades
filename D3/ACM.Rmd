---
title: "ACM"
author: "Bertita"
date: "2023-10-09"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# ACM

```{r, include=FALSE}
library(FactoMineR)
#install.packages("factoextra")
library(factoextra)
library(Matrix)
library(ggplot2)
```

```{r, include=FALSE}
load("Dades preprocessades.Rdata")

#mydata contiene toda la base de datos
mydata <- df_preprocessed
mydata[,c("AMT_INCOME_TOTAL","AMT_CREDIT","AMT_ANNUITY","AMT_GOODS_PRICE",
          "DAYS_BIRTH","TARGET","log_AMT_GOODS_PRICE","log_AMT_ANNUITY",
          "DIFF_CREDIT_GOODS")] <- NULL
dd <- mydata

```


El Multiple Correspondance Analysis, ACM en adelante, es un método de análisis factorial para variables categóricas que permite analizar relaciones entre variables, así como reducir la dimensionalidad de la base de datos seleccionando sólo aquellas variables relevantes. Para realizarlo se deben escoger unas variables activas y otras de complementarias. En este caso, como el número de variables es relativamente bajo y se consideran todas relevantes, no se considerará ninguna variable complementaria. Además, se añadirán al análisis las variables numéricas como variables suplementarias, aunque solamente aquellas que se han considerado relevantes en el ACP. 

Consideramos hacer una nueva codificación de las variables, reduciendo su longitud, de tal manera que los resultados obtenidos para el análisis se observen más claramente:

<!-- CAMBIAR EL NOMBRE DE LAS VARIABLES: -->
```{r}
# Definimos el nombre de las nuevas variables

new_variable_names <- c(
  "GDR", "INC_TYPE", "EDU_TYPE", "FAM_STAT", "OCC_TYPE", 
  "ORG_TYPE", "RR_CLIENT", "CAR_AGE", "FAM_MEMBERS", "INC_AMT", "CREDIT_AMT", "AGE_YEARS", "CREDIT_INCOME", "ANNUTY_CREDIT", "DTI_RATIO"
)


colnames(dd) <- new_variable_names
```


## Desarrollo del ACM

```{r, out.width="95%"}
res.mca <- MCA(dd, quanti.sup = c(8:15), method = "Burt")
res.mca
```
```{r}
plot(res.mca,invisible=c("ind","quali.sup"), cex=0.5)
```



En la primera figura se representan las relaciones entre las modalidades de todas las variables categóricas con las dos primeras dimensiones del MCA. Se observa que la dimensión 1 se asocia con las variables que tienen relación con la edad, como la modalidad de Pensionista y Viudo. Se aprecia que la dimensión 2 se asocia a las modalidades relacionadas con la cualificación del trabajo del individuo, con una asociación positiva entre la dimensión y la cualificación del trabajador. Por tanto, se llamará a la dimensión 1 Edad, y a la dimensión 2 como cualificiación del trabajador. 

En los gráficos obtenidos puede verse la variabilidad que expresan cada una de las variables categóricas en función de las dimensiones 1 y 2. Aquellas variables que estén más cerca del origen de coordenadas aportan muy poca información respecto a la variabilidad de los datos y, por tanto, son poco importantes. En cambio, aquellas variables más alejadas del centro aportan información más relevante.


Se representan gráficamente la inercia que explica cada una de las dimensiones generadas:

```{r}
library(ggplot2)

# Crear un data frame con los datos
data <- data.frame(Dimension = 1:nrow(res.mca$eig), 
                   Percentatge = res.mca$eig[, 2])

# Crear el gráfico de barras con ggplot2
ggplot(data, aes(x = as.factor(Dimension), y = Percentatge)) +
  geom_bar(stat = "identity", fill = "#ADD8E6") +
  ylim(0, 25) +
  labs(x = "Dimensiones",
       y = "Porcentaje de varianza explicada",
       title = "Inercia explicada por cada dimensión") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 10),
        panel.grid.major = element_blank())  # Elimina la cuadrícula de fondo


```

Si una dimensión tiene una inercia baja, significa que todas las modalidades están muy cercanas al centro de gravedad y, en consecuencia, son muy similares. A medida que aumenta la inercia, va aumentando la distancia al centro de gravedad y, por tanto, se reduce la similitud.

Para poder estudiarlo más a fondo, se realiza la siguiente tabla en la que se puede observar para cada dimensión, su valor propio, el porcentaje de varianza (o inercia) explicada, y el porcentaje de varianza (o inercia) acumulada:

```{r}
round(res.mca$eig, 2)
```
Tenemos un total de 31 dimensiones. La dimensión 1 destaca muy por encima del resto, explicando un 20.85% de la variabilidad de los datos, seguida de la dimensión 2, explicando un 9.45% de la variabilidad de los datos los datos. A partir de la dimensión 6, se ve que la gráfica se estabiliza bastante ya hasta la última dimensión.

Por tanto, en total las dos primeras dimensiones ya explican un 30.3% de la variabilidad de los datos, y se necesitan 17 dimensiones para llegar a tener una inercia acumulada por encima del 80%. 

Aunque las primeras dos dimensiones expliquen cerca del 30% de la inercia, no todos los puntos se muestran igual de bien en las dos dimensiones. La calidad de la representación se llama coseno cuadrado (cos2), que mide el grado de asociación entre categorías de variables y un eje particular.

A continuación, se representa la calidad de las categorias a partir de ajustar los colores para cada punto proyectado, tomando como criterio el valor del coseno cuadrático (cos2). Si una categoría de variable está bien representada por dos dimensiones, la suma de cos2 es cercana a uno. Para algunos de los elementos de la fila, se requieren más de dos dimensiones para representar perfectamente los datos. Se considera lo siguiente:

  - Las categorías de variables con valores bajos de cos2 se colorearán en "cian".
  - Las categorías de variables con valores medios de cos2 se colorearán en “amarillo”.
  - Las categorías de variables con valores altos de cos2 se colorearán en “rojo”.



````{r}
# Color by cos2 values: quality on the factor map
fviz_mca_var(res.mca, col.var = "cos2",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), 
             repel = TRUE, # Avoid text overlapping
             ggtheme = theme_minimal())
````
Salen muchas categorías que no están muy bien representadas por las dos primeras dimensiones. Esto implica que la posición de los puntos correspondientes en el diagrama de dispersión debe interpretarse con cierta cautela. Probablemente sea necesaria una solución de mayor dimensión. Aún así, se ha decidido no realizar el MCA de mayores dimensiones debido a la dificultad de representación gráfica. Por tanto, los resultados del MCA se analizarán con cautela, especialmente las variables poco representadas en las dos primeras dimensiones.


## Gráfico de individuos y variables

Para una primera visualización de la relación entre las variables y las observaciones en un espacio reducido de dimensiones, acudimos al gráfico biplot. Este gráfico nos facilita la interpretación de la estructura de los datos al proporcionar una visualización que muestra la relación entre variables categóricas y observaciones. 
```{r}

 fviz_mca_biplot(res.mca, col.var = "#FC4E07", col.ind = "#00AFBB", ggtheme = theme_minimal())

```

Con tal de poder analizar estos resultados de manera más precisa, se decide dividir este gráfico y analizar por una parte únicamente el gráfico de observaciones y por otra parte el gráfico de las variables categóricas.


## Gráfico de individuos

Se representa gráficamente cómo se distribuyen los individuos en función de las dos primeras dimensiones que explican un 29.99% de la variabilidad:

```{r, out.width="95%"}
fviz_mca_ind(res.mca, geom = "point", col.ind = "black")
```
A simple vista, se aprecian varios grupos de individups pero resulta difícil distinguir cuántos. Sin embargo, sí podríamos decir que los individuos se dividen en como mínimo 2 grupos. Para distinguir mejor las agrupaciones de individuos y su asociación con algunas modalidades se pasa a estudiar cada variable para observar si existe algún tipo de asociación entre ellas.

### Gráfico de los individuos según variable TARGET

A continuación representamos los mismos individuos pero coloreandolos según la variable "target", es decir, nuestra varible output, donde 1 indica aquel cliente con dificultades de pago, y 0 contrariamente:

```{r}
library(FactoMineR)

fviz_mca_ind(res.mca, geom = "point", col.ind = df_preprocessed$TARGET, 
             palette = c("orange", "purple")) 

```
Observamos como diferenciando a los individuos según si tienen dificultades o no con el pago, no hay diferencias entre grupos de individuos, por lo tanto, podemos decir que no se ve ninguna asociación entre la variable TARGET y las modalidades de las variables representadas en estas dos dimensiones. 


## Gráfico de variables

Para tener una representación aún más clara sobre las variables y su asociación, a continuación se grafican estas variables con curvas de densidad para ver aquellas zonas donde hay una mayor concentración.

```{r}
res.mca$var$contrib
```

```{r}
# Totes les variables actives
newbd<- dd[1:7]
res.mca1<-MCA(newbd, method="Burt", graph=FALSE)
cats = apply(newbd, 2, function(x) nlevels(as.factor(x)))

mca1_vars_df = data.frame(res.mca1$var$coord, Variable = rep(names(cats), cats))
mca1_obs_df = data.frame(res.mca1$ind$coord)

ggplot(data = mca1_vars_df, aes(x = Dim.1, y = Dim.2)) +
  geom_hline(yintercept = 0, colour = "gray70") +
  geom_vline(xintercept = 0, colour = "gray70") +
  geom_point(colour = "gray50", alpha = 0.7) +
  geom_density2d(colour = "gray80") +
  geom_text(data = mca1_vars_df, 
            aes(x = Dim.1, y = Dim.2, 
                label = rownames(mca1_vars_df), colour = Variable), cex = 3) +
  
  ggtitle("Gráfico MCA de variables") +
  scale_colour_discrete(name = "Variable")+ 
  xlim(-2,1.5) + 
  ylim(-1,1.5)
```


A partir de este gráfico, vemos que la dimension 1 no está explicada con claridad ya que predominan los unknowns y errores.
La dimensión 2 se ve explicada por tipo de ingresos, tipo de organización donde trabaja el cliente y actividad laboral del mismo. 
Entonces, nos explica la calidad del trabajo en relación a los ingresos obtenidos y nivel de estudios. 
Podemos ver que los mig-high skill laborers se encontran más arriba que los high skill laborer. Predecimos que esto es debido a un sesgo y que los que tienen high skill laborers necesitan menos créditos que los demás.

No podemos analizar nada preciso a partir de este gráfico y por eso pasamos a analizar por cada una de las variables.


### Gráficos de dispersión agrupado por cada variable

Con el objetivo de ver si las categorías son significativamente diferentes entre sí, se grafican gráficos de elipses alrededor de las categorías de cada una de las variables.

Se considererá que las categorías con elipses no superpuestas, es decir, separadas entre sí, son significativament diferente entre sí. Por el contrario, cuando las elipses se superponen, nos indica que hay una similitud o asociación entre categorías, es decir, no són significativamente diferentes entre ellas.

````{r}
plotellipses(res.mca,keepvar = "all", axes = c(1, 2))

for(i in 1:7){
    graf <- plotellipses(res.mca,keepvar = "all", axes = c(1, 2))[i] 
    plot(graf)
}
```

Como hemos dicho antes, analizamos por cada variable de manera individual. 
Aún así, hay muchos gráficos donde encontramos categorias superpuestas entre ellas, y que por tanto, no nos dan información significativa.


Como se ha observado que en las dos primeras dimensiones únicamente se muestra aproximadamente el 30% de la varibabilidad, para poder estudiar las categorías y su asociación más a fondo, a continuación se representan estas categorías a través de las cinco primeras dimensiones:


````{r}
# for (k in 1:7){ #para cada variable
#   for(i in 1:5) {   # 1a dimension
#   for (j in 1:5) {  # 2a dimension
#     if (i != j) {
#       grafico <- plotellipses(res.mca,keepvar = "all", axes = c(i, j))[k]
#       plot(grafico)
#     }
#   }
# }  
# }
# 

````
Hemos probado de analizar tres dimensions y no hi hay ningun gráfico significativo que nos permita extraer más información de la que ya hemos extraido con dos dimensiones. Por lo tanto, nos quedamos con los análisis sacados a partir de los gráficos de las dos primeras dimensiones. 






