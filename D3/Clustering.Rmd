---
output: pdf_document
header-includes:
  - \usepackage{fullpage} 
  - \usepackage[spanish]{babel}
  - \usepackage{fancyhdr}
  - \setlength{\headsep}{7mm} 
  - \usepackage[linktoc=page]{hyperref}
---

```{=tex}
\setlength{\headheight}{13.6pt}
\setlength{\topmargin}{-10mm}

\rhead{Minería de Datos}
\lhead{Entrega D3}
```
\pagestyle{fancy}
```{=tex}
\cfoot{\thepage}
\setcounter{page}{5}
```

```{r, include=FALSE}
#Llibreries
library(NbClust)
library(dendextend)
library(scales)
library(cluster)
library(dplyr)
library(ggplot2)
```

# Clustering

En esta sección, se emplearán diversos algoritmos de clasificación, específicamente el k-means y el Jerárquico. El propósito es asignar cada observación a un grupo correspondiente con el fin de llevar a cabo un perfilado. Este proceso implica etiquetar cada grupo con sus características más significativas, proporcionando así una descripción detallada y distintiva de cada perfil dentro de nuestros datos.

## K-means



```{r, include=FALSE}
#Lectura de los datos
load("Dades preprocessades.Rdata")

#mydata contiene toda la base de datos
mydata <- df_preprocessed
mydata[,c("AMT_INCOME_TOTAL","AMT_CREDIT","AMT_ANNUITY","AMT_GOODS_PRICE",
          "DAYS_BIRTH","TARGET","log_AMT_GOODS_PRICE","log_AMT_ANNUITY",
          "DIFF_CREDIT_GOODS")] <- NULL

Objectos <- sapply(mydata, class)
Numeriques <- names(Objectos)[which(Objectos%in%c("numeric"))]
mydata[,Numeriques] <- scale(mydata[,Numeriques])


#dd solo contiene las varibales numericas
dd <- mydata[,Numeriques]
```


El algoritmo K-means solamente permitirá utilizar las variables numéricas. Por ello se separarán los datos numéricos de la base de datos preprocesada. 

Antes de aplicar el propio algoritmo, se necesita seleccionar el número óptimo de clústeres. Para realizar esto, existen múltiples métodos, uno de ellos es el método del codo. Este consiste en aplicar el K-means para un rango de valores k y luego graficar la suma de los cuadrados de las distancias intraclúster en función de k. Para encontrar el óptimo con este método, sencillamente hace falta encontrar el "codo" del gráfico.


```{r, warning=FALSE,echo=FALSE}
wss <- (nrow(dd)-1)*sum(apply(dd,2,var))
  for (i in 2:15) wss[i] <- sum(kmeans(dd,
                                       centers=i)$withinss)
plot(1:15, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares", main = "Método del codo")
points(2, wss[2], col="red",pch = 16)
```

Como se puede apreciar en el gráfico, según el método del codo, el número óptimo de clústeres para el K-means de nuestra base de datos seria k=2.

Por lo que sigue, como existen muchos otros criterios para la selección de la k óptima, se usará la función NbClust, que permite aplicar una cantidad de 26 criterios para la selección de k, de esta manera se sabrá con mayor seguridad cuál es el óptimo real. Se grafican los resultados obtenidos por NbClust.

```{r,include=FALSE}
set.seed(1234)
nb_clustering <- NbClust(dd, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
```

```{r,echo=FALSE}
t <- table(nb_clustering$Best.nc[1,])
t <- as.data.frame(t)

ggplot(t, aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity", fill="turquoise") +
  labs(title = "Número óptimo de clústers para el K-means", x = "k", y = "nº de métodos") +
  theme(plot.title = element_text(hjust = 0.5))


tmax <-t$Var1[which(t$Freq==max(t$Freq))]
```

Como se puede apreciar en el histograma, tras haber utilizado todos los criterios, el número óptimo de clústeres que más métodos han escogido es k=`r tmax`.

Como el óptimo se encuentra en k=`r tmax`, el siguiente paso es realizar el K-means con esa k.

Después de aplicar el algoritmo y conseguir el grupo de cada individuo, se muestra el gráfico de los individuos pintados según su clase en el plano factorial de las dos primeras dimensiones del PCA, acompañado de cada una de las elipses de las clases.

```{r}
k <- 3
resultado <- kmeans(x = dd,centers = k,iter.max = 10)
clusters <- resultado$cluster
save(clusters, file = 'clust_kmeans.Rdata')
colors <- ifelse(clusters == 1, "blue",
                       ifelse(clusters == 2, "green",
                       ifelse(clusters == 3, "red", "Otros")))

pc1 <- prcomp(dd, scale = TRUE)

numer <- dd
numer <- as.data.frame(numer)
numer$clusters <- as.factor(clusters)

PC1 <- pc1$x[,1]
PC2 <- pc1$x[,2]
ggplot(numer, 
       aes(x = PC1, 
           y = PC2,color = clusters)) +
  geom_point(shape=1) +
  labs(title = "Representación de las clases en PC1-PC2", ) +
  theme(plot.title = element_text(hjust = 0.5)) +
  stat_ellipse(aes(fill = clusters), color = "black", alpha = 0.3) +
  geom_hline(yintercept = 0, colour = "red") +
  geom_vline(xintercept = 0, colour = "red")

```

Ahora bien, como se puede ver en el gráfico, no se distinguen muy bien las tres clases, ya que están unas encima de las otras. Esto puede ser consecuencia de que la clasificación se ha hecho considerando solamente las variables numéricas, es por eso que es necesario realizar un clustering jerárquico.

## Clustering jerárquico

En primera instancia, para realizar el clustering jerárquico se debe hacer primeramente un dendrograma con el método de Ward con la distancia de $$Gower^2$$. En el k-means solo se puede trabajar con las variables numéricas y en la base de datos hay variables tanto numéricas como cualitativas. La distancia de $$Gower^2$$ nos permitirá calcular las distancias tanto de las variables numéricas como de las categóricas.

Así, se calcula dicha distancia y se grafica un dendrograma:

```{r}
# Se calcula la distancia de Gower:
dist <- daisy(mydata, metric = "gower")^2

h1 <- hclust(dist, method = "ward.D2")

h1 <- as.dendrogram(h1)

plot(h1)
```

A primera vista se puede apreciar que el corte óptimo parece ser 2 clústeres. Esta cantidad de clústeres puede quedarse pequeña para los objetivos del trabajo. Consiguientemente, trataremos de tomar la decisión analíticamente, usando coeficientes que ayudan a decidir cuál es la mejor cantidad de clústeres.


Uno de ellos es el Coeficiente de Silhouette:

Los valores que retorna el Coeficiente de Silhouette van del 1 al -1. Generalmente, tomarán valores entre 1 y 0, siendo el 1 el mejor valor y 0 indicando la sobreposición de clústeres. Los valores negativos indicarían la asignación incorrecta de la muestra a los clústeres.

Lo que se hace es calcular el Coeficiente de Silhouette para diferentes cantidades de clúster y graficarlo, de manera que se cogerá el mayor valor como el número de clústeres según este criterio de Silhouette.

```{r}
avg_sil <- c()
for (k in 2:15) {
 hc_result <- hclust(dist, method = "ward.D2")
 clust <- cutree(hc_result, k)
 sil <- silhouette(clust, dist)
 avg_sil[k] <- mean(sil[,3])
}
plot(avg_sil, type = "b", xlab = "Number of clusters", ylab = "Average silhouette width")
points(2, avg_sil[1], col="red",pch = 16)
```

Como se puede ver, según el criterio de Silhouette, el número de clústeres óptimo es 2. No obstante, como existen muchos otros criterios, se usará -análogamente al K-means- la función NbClust, pero esta vez con la distancia de $$Gower$$, de este modo se consideran todas las variables.

```{r}
nb1 <- NbClust(data=NULL, distance=NULL, diss = dist , method = "ward.D2", index ="frey")
nb2 <- NbClust(data=NULL, distance=NULL, diss = dist , method = "ward.D2", index ="mcclain")
nb3 <- NbClust(data=NULL, distance=NULL, diss = dist , method = "ward.D2", index ="cindex")
nb4 <- NbClust(data=NULL, distance=NULL, diss = dist , method = "ward.D2", index ="silhouette")
nb5 <- NbClust(data=NULL, distance=NULL, diss = dist , method = "ward.D2", index ="dunn")


vec <- c(nb1$Best.nc[1],nb2$Best.nc[1],nb3$Best.nc[1],nb4$Best.nc[1],nb5$Best.nc[1])
tabl <- table(vec)
tabl <- as.data.frame(tabl)

ggplot(tabl, aes(x = vec, y = Freq)) +
  geom_bar(stat = "identity", fill="turquoise") +
  labs(title = "Número óptimo de clústers para el Jerárquico", x = "k", y = "nº de métodos") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
plot(h1)
```

Con el dendrograma anterior, se confirma que el mejor corte (después de k = 2) es k = 3 y k = 5. Se divide el mismo dendrograma en k = 5 grupos:

```{r}
objecto2 <- cutree(h1, 5)
# Se aplican los resultados a los datos no normalizados 
mydata$cluster2 <- objecto2
colors <- c("red", "green", "blue","purple","orange","black")
colors_dend <- color_branches(h1, labels = objecto2, k = 5, col = colors)
plot(colors_dend)
```

Se escoge k=5 para hacer un perfilamiento de grupos detallado.


```{r}
save(mydata, file = 'data_clust.Rdata')
```

