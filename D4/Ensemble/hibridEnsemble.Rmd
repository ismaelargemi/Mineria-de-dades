---
output: pdf_document
header-includes:
  \usepackage{fullpage} 
  \usepackage[spanish]{babel}
  \setlength{\headsep}{7mm} 
  \usepackage[linktoc=page]{hyperref}
  \usepackage{fancyhdr}
  \usepackage{floatrow}
  \floatsetup[figure]{capposition=top}
  \floatsetup[table]{style=plaintop}
  \usepackage{float} 
  \floatplacement{figure}{H} 
  \newcommand{\beginsupplement}{\setcounter{table}{1}\renewcommand{\thetable}{\arabic{table}} \setcounter{figure}{1} \renewcommand{\thefigure}{\arabic{figure}}}
---

\pagenumbering{arabic}

```{=tex}
\setlength{\headheight}{13.6pt}
\setlength{\topmargin}{-10mm}

\rhead{Minería de Datos}
\lhead{Entrega D3}
```
\pagestyle{fancy}
```{=tex}
\cfoot{\thepage}
\setcounter{page}{1}
```
\beginsupplement

```{r, include=FALSE}
knitr::opts_chunk$set(comment="")
```

```{r, include=FALSE}
library(dplyr)
library(kableExtra)
library(caret)
library(caretEnsemble)
library(car)
```

```{r}
generar_tabla_metricas <- function(predicciones, nombre_top_model) {
  conf_matrix <- confusionMatrix(predicciones, TARGET_test, positive = "1")
  accuracy <- conf_matrix$overall["Accuracy"]
  specificity <- conf_matrix$byClass["Specificity"]
  sensitivity <- conf_matrix$byClass["Sensitivity"]
  f1_score <- conf_matrix$byClass["F1"]
  
  metricas <- data.frame(Accuracy = accuracy,
                         Specificity = specificity,
                         Sensitivity = sensitivity,
                         F1 = f1_score)
  
  rownames(metricas) <- nombre_top_model  
  
  return(metricas)
}
```

```{r, warning=FALSE}
load("~/GitHub/Mineria-de-dades/D4/Ensemble/Pruebas.RData")
Train <- train_balanceado
Test <- validation_desbalanceado

TARGET_train <- Train[["TARGET"]]
TARGET_test <- Test[["TARGET"]]

Train <- Train %>%
  select(-TARGET) %>%
  mutate(across(where(is.factor), as.numeric))

Test <- Test %>%
  select(-TARGET) %>%
  mutate(across(where(is.factor), as.numeric))

Train$TARGET <- TARGET_train
Test$TARGET <- TARGET_test
```

# Ensemble Híbrido

En este último apartado del trabajo se implementan dos métodos de ensamblaje híbrido.

Un ensamblaje híbrido consiste en la combinación de diferentes técnicas de ensamblaje en un solo marco de trabajo para mejorar el rendimiento predictivo del modelo. Este enfoque se basa en la premisa de que la combinación de múltiples modelos puede proporcionar predicciones más precisas y robustas que cualquier modelo individual.


EXPLICAR MEJOR
El método de ensamblaje empleado utiliza un modelo lineal para realizar la regresión de las predicciones de nuestros modelos, con el objetivo de obtener más información sobre la contribución de cada modelo y también para observar si hay diferencias entre estos dos métodos de ensamblaje.

```{r}
set.seed(123) 
folds <- createFolds(Train$TARGET, k = 10)

control <- trainControl(method = "cv", number = 10, classProbs=TRUE, savePredictions="final", index = folds)

lista <- c('knn', 'lda', 'qda', 'svmLinear', 'nb', 'rpart', 'rf','xgbTree')

Train$TARGET <- as.factor(make.names(as.character(Train$TARGET)))

models <- caretList(
  TARGET~., data=Train, trControl=control, methodList=lista
)
```


```{r}
result <- caretEnsemble(models)
sr <- summary(result)
```

REDACCIÓN DE LOS PESOS

```{r}
predicciones_wgt <- predict(result, newdata=Test)
```

Comparamos las metricas de validacion de ambos procesos de ensamblaje con un conjunto de datos Test balanceado para observar overfitting:

```{r, warning=FALSE}
metricas_rf <- generar_tabla_metricas(predicciones_rf, "Random Forest")
levels(predicciones_wgt) <- gsub("X", "", levels(predicciones_wgt))
metricas_wgt <- generar_tabla_metricas(predicciones_wgt, "Weighted")

tabla_metricas_completa <- rbind(metricas_wgt, metricas_rf)


kbl(as.matrix(tabla_metricas_completa), 
    caption = "Métricas de validación", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```


Vemos como ambos metodos obtienen resultados en las metricas de validacion muy similares, mostrando un buen desempeño en las predicciones, aun mostrando un poco de descompensacion en la especificidad y la sensibilidad.....

Ahora hacemos la prueba ácida:
```{r}
Test <- validation_desbalanceado

TARGET_test <- Test[["TARGET"]]

Test <- Test %>%
  select(-TARGET) %>%
  mutate(across(where(is.factor), as.numeric))

Test$TARGET <- TARGET_test

predicciones_wgt <- predict(result, newdata=Test)
```

Comparamos las metricas de validacion de ambos procesos de ensamblaje con un conjunto de datos Test balanceado para observar overfitting:

```{r, warning=FALSE}
metricas_rf <- generar_tabla_metricas(predicciones_rf, "Random Forest")
levels(predicciones_wgt) <- gsub("X", "", levels(predicciones_wgt))
metricas_wgt <- generar_tabla_metricas(predicciones_wgt, "Weighted")

tabla_metricas_completa <- rbind(metricas_wgt, metricas_rf)


kbl(as.matrix(tabla_metricas_completa), 
    caption = "Métricas de validación", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Conclusiones
