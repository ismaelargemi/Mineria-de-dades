---
output: pdf_document
header-includes:
  \usepackage{fullpage} 
  \usepackage[spanish]{babel}
  \setlength{\headsep}{7mm} 
  \usepackage[linktoc=page]{hyperref}
  \usepackage{fancyhdr}
  \usepackage{floatrow}
  \floatsetup[figure]{capposition=top}
  \floatsetup[table]{style=plaintop}
  \usepackage{float} 
  \floatplacement{figure}{H} 
  \newcommand{\beginsupplement}{\setcounter{table}{81}\renewcommand{\thetable}{\arabic{table}} \setcounter{figure}{155} \renewcommand{\thefigure}{\arabic{figure}}}
---

\pagenumbering{arabic}

```{=tex}
\setlength{\headheight}{13.6pt}
\setlength{\topmargin}{-10mm}

\rhead{Minería de Datos}
\lhead{Entrega D3}
```
\pagestyle{fancy}
```{=tex}
\cfoot{\thepage}
\setcounter{page}{196}
```
\beginsupplement

```{r, include=FALSE}
knitr::opts_chunk$set(comment="")
```

# XGBoost

XGBoost es un modelo basado en árboles de decisión y es una mejora de otros métodos de ensamblaje como el Random Forest. El algoritmo utiliza varios métodos de optimización para mejorar la precisión y controlar el sobre ajuste.

El proceso de XGBoost comienza con una predicción inicial y luego calcula los residuos, que son las diferencias entre las predicciones y los valores observados. Luego, crea un árbol de decisión con estos residuales y continúa este proceso, construyendo árboles secuenciales que aprenden de los errores del árbol anterior.

Este modelo se puede ajustar según ciertos parámetros. Para encontrar la mejor combinación de parámetros. Se llevará a cabo validación cruzada en el conjunto de entrenamiento con el fin de extraer los mejores hiperparámetros. Posteriormente, se aplicará el mejor modelo obtenido en nuestro conjunto de prueba para su validación.

```{r, include=FALSE, warning=FALSE}
library(caret)
library(dplyr)
library(xgboost)
library(kableExtra)
library(parallel)
library(DiagrammeR)
library(ggplot2)
library(pROC)
```


```{r,include=FALSE}
#OH Encoding Train
load("Pruebas.Rdata")

Train <- train_balanceado

TARGET <- as.vector(Train$TARGET)
Train$TARGET <- NULL #TARGET tiene q ser factor 2 niveles, no numeric

Objectos <- sapply(Train, class)
Categoriques <- names(Objectos)[which(Objectos%in%c("factor"))]

Cat <- predict(dummyVars("~ .", data = Train[,Categoriques]), newdata = Train[,Categoriques])

Train[,Categoriques] <- NULL

Train <- cbind(Train, Cat)

#OH Encoding Test balanceado
Test <- validation_balanceado

TARGET_test <- as.vector(Test$TARGET)
Test$TARGET <- NULL #TARGET tiene q ser factor 2 niveles, no numeric

Cat <- predict(dummyVars("~ .", data = Test[,Categoriques]), newdata = Test[,Categoriques])

Test[,Categoriques] <- NULL

Test <- cbind(Test, Cat)
```

```{r, include=FALSE}
grid <- expand.grid(
  nrounds = c(100), # Número de rondas de boosting
  max_depth = seq(3,15), # Profundidad máxima de los árboles
  eta = c(0.001,0.01,0.1,0.2,0.3), # Tasa de aprendizaje
  gamma = c(0), # Parámetro de regularización
  colsample_bytree = c(1), # Fracción de columnas para cada árbol
  min_child_weight = c(1), # Suma mínima de pesos de las instancias necesarias en un hijo
  subsample = c(1) # Fracción de instancias para entrenar cada árbol
)

control <- trainControl(method = "cv", number = 10, verboseIter = FALSE, allowParallel = TRUE)
```

```{r, include=FALSE}
set.seed(1234)
model <- train(
  x = Train, 
  y = TARGET,
  trControl = control,
  tuneGrid = grid,
  method = "xgbTree",
  verbosity = 0
)
```

En la validación cruzada usamos Grid Search. Se busca la mejor combinación de parámetros (como tasa de aprendizaje, número de rondas y profundidad del árbol) probando múltiples valores. 

Esto implica entrenar y evaluar el modelo con cada combinación para encontrar la configuración que maximice el rendimiento. 

Posteriormente, se grafica cómo varía el desempeño del modelo en función de estos valores, lo que ayudará a identificar la configuración óptima para obtener mejores resultados en otros conjuntos de datos.

```{r, echo=FALSE}
ggplot(model)
```
Los valores óptimos de nuestros parámetros según la función podrían ser:

- Número de rondas de boosting `r model$bestTune$nround`
- Profundidad de árbol `r model$bestTune$max_depth `
- Tasa de aprendizaje `r model$bestTune$eta `

Aunque estamos obteniendo estos resultados, queremos estudiar más profundamente en qué valores fijar estos parámetros para mejorar la eficiencia y sencillez de nuestro árbol. Queremos observar cuándo convergen nuestros valores.

Así se propone:

La profundidad del árbol fijada en 7, ya que, según se puede observar en el gráfico anterior, la precisión parece estabilizarse a partir de esa profundidad. Por tanto, los niveles posteriores del árbol no aportarían información adicional que contribuyera a mejorar la precisión del modelo.


```{r,include=FALSE}
dtrain <- xgb.DMatrix(data = as.matrix(Train), label = TARGET)
dtest <- xgb.DMatrix(data = as.matrix(Test), label = TARGET_test)
params <- list(
  objective = "binary:hinge",
  eval_metric = "error",
  eta = 0.1,
  max_depth = 7,
  subsample = 1,
  colsample_bytree = 1
)

#model1 <- xgb.train(dtrain,label=TARGET, params=params, nrounds = 200)
#xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 200, watchlist = list(train = dtrain, test = dtest))

xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100)
```

```{r,echo=FALSE, warning=FALSE}
ggplot(data = xgb_model$evaluation_log, aes(x = 1:length(train_error), y = train_error)) +
  geom_line(color = "black") +
  geom_point(color = "red", size = 2, shape = 19) +
  geom_vline(xintercept=50, linetype="dashed", color="blue")+
  labs(x = "Número de árboles", y = "Error")
```
Por otro lado, en este gráfico se puede observar la evolución del error en función del número de iteraciones. Nos muestra como este valor tiende a estabilizarse a partir de la iteración 50. Así se fija el número de rondas en este valor.

Se escoge como valor de eta 0.1. Aun siendo un valor elevado que podría generar overfitting, queremos probar si obtenemos un buen modelo.

Para verificar que nuestro modelo no tiene overfitting, antes de aplicar la prueba ácida, usaremos un conjunto de Test balanceado con nuestro modelo.

```{r,include=FALSE}
grid_opt <- expand.grid(
  nrounds = 50, 
  max_depth = c(7), 
  eta = 0.1, 
  gamma = c(0), 
  colsample_bytree = c(1), 
  min_child_weight = c(1), 
  subsample = c(1) 
)
xgbTree <- train(x=Train, y=TARGET, method="xgbTree",
                    metric="Accuracy", trControl=trainControl(method="none"),tuneGrid=grid_opt)

predictxgbTree <- predict(xgbTree,Test)
conf_matrix <- confusionMatrix(predictxgbTree, as.factor(TARGET_test), positive="1")
```

```{r, warning=FALSE, echo=FALSE, fig.cap = "Métricas de validación, prueba balanceada", fig.show='hold',out.width="75%",out.height="75%"}
accuracy <- conf_matrix$overall["Accuracy"]
specificity <- conf_matrix$byClass["Specificity"]
sensitivity <- conf_matrix$byClass["Sensitivity"]

metricas <- data.frame(Accuracy=accuracy,Specificity=specificity,Sensitivity=sensitivity)
metricas <- as.matrix(metricas)
kbl(metricas, 
    caption = "Métricas de validación", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Comparando Accuracy, en train obtenemos un 0.8179768 y en test 0.7910714, lo que verifica que el modelo con estos parámetros no genera overfitting. Ahora aplicaremos la prueba ácida, con datos test desbalanceados.


```{r,include=FALSE}
Test <- validation_desbalanceado

TARGET_test <- as.vector(Test$TARGET)
Test$TARGET <- NULL 

Cat <- predict(dummyVars("~ .", data = Test[,Categoriques]), newdata = Test[,Categoriques])

Test[,Categoriques] <- NULL

Test <- cbind(Test, Cat)

predictxgbTree <- predict(xgbTree,Test)
conf_matrix <- confusionMatrix(predictxgbTree, as.factor(TARGET_test), positive="1")
```

```{r, warning=FALSE, echo=FALSE, fig.cap = "Métricas de validación, prueba desbalanceada", fig.show='hold',out.width="75%",out.height="75%"}
accuracy <- conf_matrix$overall["Accuracy"]
specificity <- conf_matrix$byClass["Specificity"]
sensitivity <- conf_matrix$byClass["Sensitivity"]

metricas <- data.frame(Accuracy=accuracy,Specificity=specificity,Sensitivity=sensitivity)
metricas <- as.matrix(metricas)
kbl(metricas, 
    caption = "Métricas de validación", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

**Resultados**

Los resultados del modelo XGBoost muestran un rendimiento sólido en la clasificación, con una precisión  en la predicción del `r accuracy` %.

Destaca la alta especificidad del modelo, alcanzando un `r specificity`, lo que indica su capacidad para identificar correctamente los verdaderos negativos. Además, la sensibilidad es del `r sensitivity`, lo que nos muestra su eficacia en la identificación de casos positivos, aunque es mejorable. 

Esta evaluación sugiere que el modelo no presenta overfitting, ya que se presentan valores similares en la precisión y otros.

Así, el árbol conseguido en el modelo es el siguiente:
```{r, echo=FALSE, warning=FALSE}
grid_opt <- expand.grid(
nrounds = 50,
max_depth = c(7),
eta = 0.1,
gamma = c(0),
colsample_bytree = c(1),
min_child_weight = c(1),
subsample = c(1)
)
xgbTree <- train(x=Train, y=TARGET, method="xgbTree",
                 metric="Accuracy", trControl=trainControl(method="none"),tuneGrid=grid_opt)

# xgb.plot.tree(model=xgbTree$finalModel, trees = 49)
```

![]("C:\Users\oscar\Documents\GitHub\Mineria-de-dades\D4\XGB\xgbARBOL.png")

Los niveles iniciales de nuestro árbol, desde la raíz, revelan las variables más relevantes para la clasificación de los clientes. Observamos cómo estas variables y sus respectivos puntos de corte se despliegan de la siguiente manera:

Primer nivel: Se realiza una división utilizando la característica RATIO_ANNUITY_CREDIT (Ratio entre la anuidad del préstamo y el crédito total solicitado), con un umbral de < 0.115123019. Si se cumple esta condición, se avanza al segundo nivel; de lo contrario, se llega a una hoja.

La hoja resultante de esta división tiene una cobertura de 1.09869659, lo que significa que esta regla se aplica a aproximadamente 1.1 unidades de datos.

Segundo nivel: Se divide utilizando la característica log_AMT_CREDIT (logaritmo del importe de crédito del préstamo), con un umbral de < 11.8223429. Si se cumple la condición, se avanza al tercer nivel A; de lo contrario, se accede al nivel B.

Tercer nivel A: Se divide utilizando la característica OWN_CAR_AGE (edad del coche), con un umbral de < 16.5.

Tercer nivel B: Se divide utilizando la característica CNT_FAM_MEMBERS (número de miembros en la familia), con un umbral de < 4.8.

La característica CNT_FAM_MEMBERS muestra un valor de Ganancia (Gain) de 1.77633166 al dividir los datos. Esto indica que contribuye significativamente a la reducción de la impureza o mejora la separación de los datos en comparación con otras características en este nivel específico del árbol.

Este patrón continúa a través de los niveles subsiguientes hasta alcanzar el séptimo nivel. Es importante destacar que la siguiente variable en orden de importancia es la edad.

**Conclusiones:**

Los resultados del modelo XGBoost revelan un rendimiento consistente en la tarea de clasificación, particularmente notorio por su alta especificidad. Consideramos que este modelo es eficaz para clasificar nuestros datos según nuestros objetivos, aunque nos interesa más una sensibilidad elevada que una alta especificidad. Una mayor sensibilidad implica tener más falsos negativos que falsos positivos, lo cual es crucial al considerar si un cliente será moroso o no.
