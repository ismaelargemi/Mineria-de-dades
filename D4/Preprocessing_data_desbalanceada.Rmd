---
title: "Preprocessing"
author: "Iker Meneses Sales"
date: "20-11-2023"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include = F, warning = F, message = F}
library(ggplot2)
library(dplyr)
library(corrplot)
library(cluster)
library(naniar)
library(kableExtra)
library(readxl)

df_final = read.csv2("C:/Users/iker1/Downloads/data_desbalanceada.csv", stringsAsFactors = T)
rownames(df_final) = df_final[,1]
df_final = df_final[,-1]
df_final$REGION_RATING_CLIENT = factor(df_final$REGION_RATING_CLIENT)
df_final$TARGET = factor(df_final$TARGET)
```

Para realizar el preprocesamiento de los datos, será óptimo seguir los pasos propuestos por Karina Gibert con el objetivo de desarrollar correctamente el KDD y, así, obtener conclusiones óptimas a partir de nuestros datos.

Para ello, seguiremos 4 grandes bloques:

-   Limpieza de datos y estandarización de formato
-   Detección y tratamiento de missings
-   Detección y tratamiento de outliers
-   Feature Engineering

# Limpieza de datos y estandarización de formato

Una vez hemos realizado la descriptiva preprocessing y hemos identificado el número de valores missing en nuestra base de datos, es óptimo analizar todas las variables una a una, así como algunas variables categóricas a las cuales se les puede reducir el número de categorías.

Para empezar, se puede apreciar que la variable `OCCUPATION_TYPE` tiene un total de 18 categorías:

```{r, echo = F}
a = table(df_final$OCCUPATION_TYPE, useNA = "always")

kbl(a, col.names = c("Categoría", "Frecuencia"),
    caption = "Distribución inicial de la variable OCCUPATION TYPE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

```


Una buena idea sería combinar algunas categorías con el objetivo de reducir el número de categorías y, además, aumentar el número de individuos por categoría. Seguidamente, se muestran los cambios realizados, donde se han agrupado todos los individuos en 5 categorías en función del capital humano empleado para su puesto:

-   Low skill laborers: Engloba las categorías de "security staff", "cooking staff", "cleaning staff", "drivers", "low skill laborers", "waiters staff".

-   Low-mid skill laborers: Engloba las categorías de "secretaries", "private service staff" y "laborers".

-   Mid skill laborers: Engloba las categorías de "accountants", "HR staff" y "sales staff".

-   Mid-high skill laborers: Engloba las categorías de "IT staff", "realty agents" y "core staff".


-   High skill staff: Engloba las categorías de "high skill tech staff", "managers" y "medicine staff".

```{r, echo = F}
df_final$OCCUPATION_TYPE = as.character(df_final$OCCUPATION_TYPE)

df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Security staff")] = "Low skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Cooking staff")] = "Low skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Cleaning staff")] = "Low skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Drivers")] = "Low skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Low-skill Laborers")] = "Low skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Waiters/barmen staff")] = "Low skill laborers"

df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Secretaries")] = "Low-mid skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Private service staff")] = "Low-mid skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Laborers")] = "Low-mid skill laborers"

df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Accountants")] = "Mid skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "HR staff")] = "Mid skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Sales staff")] = "Mid skill laborers"

df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "IT staff")] = "Mid-high skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Realty agents")] = "Mid-high skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Core staff")] = "Mid-high skill laborers"

df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "High skill tech staff")] = "High skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Managers")] = "High skill laborers"
df_final$OCCUPATION_TYPE[which(df_final$OCCUPATION_TYPE == "Medicine staff")] = "High skill laborers"

df_final$OCCUPATION_TYPE = factor(df_final$OCCUPATION_TYPE)

a = table(df_final$OCCUPATION_TYPE, useNA = "always")

kbl(a, col.names = c("Categoría", "Frecuencia"),
    caption = "Distribución final de la variable OCCUPATION TYPE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

```

Este proceso lo repetiremos con la variable `ORGANIZATION_TYPE`:

```{r, echo = F}
a = table(df_final$ORGANIZATION_TYPE, useNA = "always")

kbl(a, col.names = c("Categoría", "Frecuencia"),
    caption = "Distribución inicial de la variable ORGANIZATION TYPE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

```

Como se puede apreciar, en este caso disponemos de muchísimas categorías, pero es de destacar la categoría XNA, la cual deberíamos sustituir a NA, para después poder imputarle algún valor. Así pues, se ha agrupado cada categoría profesional en función del sector al que se dedica el individuo. Así, la distribución final es la siguiente:

```{r, echo = F}
a = as.character(df_final$ORGANIZATION_TYPE)

a[which(startsWith(a,"Trade"))] = "Trade and telecom"
a[which(startsWith(a,"Transp"))] = "Transport"
a[which(startsWith(a,"Indus"))] = "Industry and construction"
a[which(startsWith(a, "Business"))] = "Business and bank"
a[which(startsWith(a,"Transp"))] = "Transport"
a[which(a == "XNA")] = NA

a[which(a == "Construction")] = "Industry and construction"

a[which(a == "School")] = "Education"
a[which(a == "University")] = "Education"
a[which(a == "Kindergarten")] = "Education"
a[which(a == "Culture")] = "Other"

a[which(a == "Security Ministries")] = "Public services"
a[which(a == "Police")] = "Public services"
a[which(a == "Emergency")] = "Public services"
a[which(a == "Military")] = "Public services"


a[which(a == "Postal")] = "Other"
a[which(a == "Cleaning")] = "Personal services"
a[which(a == "Restaurant")] = "Other"
a[which(a == "Advertising")] = "Other"
a[which(a == "Legal Services")] = "Personal services"
a[which(a == "Realtor")] = "Personal services"
a[which(a == "Hotel")] = "Personal services"
a[which(a == "Insurance")] = "Business and bank"
a[which(a == "Bank")] = "Business and bank"


a[which(a == "Housing")] = "Personal services"
a[which(a == "Services")] = "Personal services"
a[which(a == "Security")] = "Personal services"


a[which(a == "Mobile")] = "Other"
a[which(a == "Telecom")] = "Trade and telecom"
a[which(a == "Religion")] = "Other"
a[which(a == "Agriculture")] = "Other"


a[which(a == "Electricity")] = "Public services"
a[which(a == "Government")] = "Public services"


b = table(a)

kbl(b, col.names = c("Categoría", "Frecuencia"),
    caption = "Distribución final de la variable ORGANIZATION TYPE", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

df_final$ORGANIZATION_TYPE = factor(a)
```

Ahora, esta variable pasa a tener 10 categorías, las cuales representan los diferentes sectores presentes en la economía presente hoy en día.

Así pues, el resto de variables tienen una uniformidad evidente: se puede apreciar cómo las variables categóricas presentan un número de categorías pequeño y, por parte de las variables numéricas, todas están expresadas en las mismas unidades, de forma que no habrá problemas con la manipulación de éstas.


# Detección y tratamiento de missings

Para este apartado, trataremos de identificar aquellos valores desconocidos y valorar sobre su aleatoriedad para, posteriormente, imputar valores. Para empezar, es de destacar cómo hay 47 individuos con un coche de 64 años y 11 con un coche de 65. Si nos fijamos en la distribución de esta variable, es muy extraño que haya tantos individuos con valores atípicos, ya que el siguiente valor máximo es 46. Así, se potará por imputar valores nulos a estos individuos.

```{r, echo = F}
df_final$OWN_CAR_AGE[which(df_final$OWN_CAR_AGE == 63)] = NA
df_final$OWN_CAR_AGE[which(df_final$OWN_CAR_AGE == 64)] = NA
df_final$OWN_CAR_AGE[which(df_final$OWN_CAR_AGE == 65)] = NA
```

Seguidamente, pasaremos a imputar diferentes valores a aquellas variables donde hay observaciones sobre las cuales se desconocen sus valores reales. Este paso es necesario, ya que el hecho de disponer de valores desconocidos (también conocidos como NA) dificulta el análisis posterior de la variable. 

Una vez hemos recategorizado todas aquellas variables que presentaban problemas, el número de NA por variables es el siguiente:

```{r, echo = F}
a = colSums(is.na(df_final))
a = matrix(a, nrow = 15, ncol=2)
a[,1] = names(colSums(is.na(df_final)))

kbl(a, col.names = c("Categoría", "Frecuencia"),
    caption = "Missings por variable", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))

```

Una vez tenemos identificados todos los valores missing de nuestra base de datos, será necesario identificar si éstos son completamente aleatorios (MCAR), aleatorios (MAR), o no aleatorios (MNAR). Para ello, realizaremos el test de Little, el cual indica si los missings disponibles en la base de datos son fruto del azar o si siguen un patrón.

Para este test, diremos que los datos no siguen un patrón si no se rechaza hipótesis nula o, alternativamente, si no encuentra patrones entre los missings. Así pues, este es el resultado:

```{r, echo = F}
little = mcar_test(df_final)

a = matrix(nrow = 2,ncol = 4)

a[1,] = names(data.frame(little))
a[2,] = unlist(little)

kbl(a,
    caption = "Test de Little", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))



```

Como se puede apreciar, el algoritmo ha detectado 7 patrones entre los valores missing, de forma que no se puede decir que hay un patrón aleatorio, de forma que calificaremos nuestros valores missing como MNAR.

Seguidamente, imputaremos los valores por los tres métodos de imputación conocido, pero antes de imputar los valores numéricos, será necesario pasar los NA a categoría `unknown`.

```{r, echo = F}
ind = which(is.na(df_final$OCCUPATION_TYPE))

df_final$OCCUPATION_TYPE = as.character(df_final$OCCUPATION_TYPE)
df_final$OCCUPATION_TYPE[ind] = "Unknown"
df_final$OCCUPATION_TYPE = factor(df_final$OCCUPATION_TYPE)



ind = which(is.na(df_final$ORGANIZATION_TYPE))

df_final$ORGANIZATION_TYPE = as.character(df_final$ORGANIZATION_TYPE)
df_final$ORGANIZATION_TYPE[ind] = "Unknown"
df_final$ORGANIZATION_TYPE = factor(df_final$ORGANIZATION_TYPE)
```

Seguidamente, toca imputar los NA disponibles en las variables numéricas de nuestros datos. Para ello, utilizaremos tres métodos distintos: kNN, MiMMi y MICE. Posteriormente, se comparará la imputación entre estos métodos y se seleccionará el método que resulte una distribución más parecida a la original antes de imputar.

```{r, include = F}
df_est = df_final
df_knn = df_final
df_mimmi = df_final
df_mice = df_final
```

### Imputación por criterios estadísticos

En este caso, el objetivo será imputar en función de criterios estadísticos básicos. Para ello, se procederá a imputar valores en función de la media estadística o algún otro estadístico central de distribución.

```{r, include = F}
df_est$OWN_CAR_AGE[which(is.na(df_est$OWN_CAR_AGE))] = mean(df_est$OWN_CAR_AGE, na.rm = T)

df_est$AMT_GOODS_PRICE[which(is.na(df_est$AMT_GOODS_PRICE))] = mean(df_est$AMT_GOODS_PRICE, na.rm = T)
```


### Imputación por kNN

El algoritmo K-Nearest Neighbors (KNN), es un método de clasificación supervisada, que utiliza la proximidad para hacer clasificaciones o predicciones sobre un punto de datos desconocido. El algortimo, utiliza un hiperparámetro llamado "k", que representa el número de vecinos más cercanos y el cual se ha obtenido mediante el cálculo de $k= \sqrt{n}$. 

```{r, message = F, warning = F, include = F}
df_knn <- select_if(df_final, is.numeric)
n <- nrow(df_knn)

library(class)
```

A continuación, se crean dos objetos: `fullVariables`, que corresponde a las variables que no presentan ningún dato faltante y `uncompleteVars`, que guarda las variables con missings.

```{r, include = F}
fullVariables <- names(df_knn)[which(colSums(is.na(df_knn))==0)] 
aux <- df_knn[,fullVariables]
dim(aux)
names(aux)

uncompleteVars<- names(df_knn)[which(colSums(is.na(df_knn))>0)] 

for (k in uncompleteVars){
  aux1 <- aux[!is.na(df_knn[,k]),]
  dim(aux1) 
  aux2 <- aux[is.na(df_knn[,k]),]
  dim(aux2)
  
  RefValues<- df_knn[!is.na(df_knn[,k]),k]
  knn.values = knn(aux1,aux2,RefValues, k = round(sqrt(n)) )  
  df_knn[is.na(df_knn[,k]),k] = as.numeric(as.character(knn.values))
  fullVariables<-c(fullVariables, k)
  aux<-df_knn[,fullVariables]
}

df_preprocessed_knn = data.frame(aux,select_if(df_final, is.factor))

```

Como se puede observar, se obtiene la imputación de los valores faltantes en el dataframe `df_knn` utilizando el algoritmo descrito previamente.


### Imputación por MiMMi

La imputación por MiMMi se realiza utilizando un enfoque basado en clústeres y se utiliza la distancia de Gower como métrica de distancia para medir la similitud entre observaciones.

La función uncompleteVar se define para verificar si hay valores faltantes (representados como NA) en un vector dado.

La función Mode se define para calcular la moda de un vector. Esta función se utiliza más adelante para imputar valores faltantes en variables categóricas.

```{r, echo = FALSE, warning = F, message = F}
# install.packages("StatMatch")
library(cluster)
require(StatMatch)

# assume missings represented with NA
uncompleteVar <- function(vector){any(is.na(vector))}

Mode <- function(x) 
{
  x <- as.factor(x)
  maxV <- which.max(table(x))
  return(levels(x)[maxV])
}
```


Se define la función MiMMi.

```{r, echo = FALSE}
MiMMi <- function(data, priork=-1)
{
  # Identify columns without missings
  colsMiss <- which(sapply(data, uncompleteVar))
  if(length(colsMiss) == 0){
    print("Non missing values found")
    out <- data
  } else {
    K <- dim(data)[2]
    colsNoMiss <- setdiff(c(1:K), as.vector(colsMiss))

    #cluster with complete data
    dissimMatrix <- daisy(data[ , colsNoMiss], metric = "gower", stand = TRUE)
    distMatrix <- dissimMatrix^2

    hcdata <- hclust(distMatrix, method = "ward.D2")
    plot(hcdata)

    if(priork == -1){
      nk <- readline("See the dendrogramm and enter a high number of clusters (must be a positive integer). k: ")
      nk <- as.integer(nk)
    } else {nk <- priork}

    partition <- cutree(hcdata, nk)

    CompleteData <- data
    # només cal per tenir traça de com s'ha fet la substitució
    newCol <- K+1
    CompleteData[ , newCol] <- partition
    names(CompleteData)[newCol] <- "ClassAux"

    setOfClasses <- as.numeric(levels(as.factor(partition)))
    imputationTable <- data.frame(row.names = setOfClasses)
    p <- 1

    for(k in colsMiss)
    {
      # Files amb valors utils
      rowsWithFullValues <- !is.na(CompleteData[,k])

      # Calcular valors d'imputació
      if(is.numeric(CompleteData[,k]))
      {
        imputingValues <- aggregate(CompleteData[rowsWithFullValues,k], by = list(partition[rowsWithFullValues]), FUN = mean)
      } else {
        imputingValues <- aggregate(CompleteData[rowsWithFullValues,k], by = list(partition[rowsWithFullValues]), FUN = Mode)
      }

      # Impute
      for(c in setOfClasses)
      {
        data[is.na(CompleteData[,k]) & partition == c,k] <- round(imputingValues[c,2],0)
      }

      # Imputation Table
      imputationTable[,p] <- imputingValues[,2]
      names(imputationTable)[p] <- names(data)[k]
      p <- p+1
    }

    rownames(imputationTable) <- paste0("c", 1:nk)
    out <- new.env()
    out$imputedData <-data
    out$imputation <- imputationTable
  }
  return(out)
}
```

Se usa la función MiMMi y se obtienen los resultados imputados.

```{r, include = F}
dimpute<-MiMMi(df_mimmi, priork = 3)

# table of imputation values used
#dimpute$imputation

# imputed dataset
df_preprocessed_mimmi <- dimpute$imputedData
```

### Imputación por MICE

Por último, se recurrirá a imputar a través del MICE como último método de imputación de valores numéricos. El MICE (Multiple Imputation by chained Equations) se basa en un método iterativo a partir del cual se resuelven ecuaciones consecutivamente con el objetivo de imputar valores de la forma más aproximada posible. Así pues, es momento de imputarlo:


```{r, include=F, message=F, warning = F}
library(mice)
factor_vars = c("CODE_GENDER", "NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE", 
                "NAME_FAMILY_STATUS", "OCCUPATION_TYPE", "ORGANIZATION_TYPE", 
                "REGION_RATING_CLIENT", "TARGET")

num_vars = c("AMT_INCOME_TOTAL", "AMT_CREDIT", "AMT_ANNUITY", "DAYS_BIRTH",
             "OWN_CAR_AGE", "AMT_GOODS_PRICE", "CNT_FAM_MEMBERS")

x = c(factor_vars, num_vars)

dd <- df_mice[, x]
mice_data = mice(dd, method = "cart")
df_preprocessed_mice = complete(mice_data,5)
```

```{r, include = F, echo = F}
summary(df_preprocessed_mice)
colSums(is.na(df_preprocessed_mice))
```


### Decisión del método de imputación elegido

Llegados a este punto, en el momento de seleccionar el método de imputación elegido para el método de imputación final. En nuestro caso, como únicamente disponemos de dos variables numéricas con missings, podemos comparar la función de densidad de los datos originales contra los imputados por cada método. Así pues, vamos a mirar variable por variable:

```{r, echo = F, include = F}
bbc_style = function() 
{
  font <- "Helvetica"
  ggplot2::theme(plot.title = ggplot2::element_text(family = font, 
                                                    size = 18, face = "bold", color = "#222222"), plot.subtitle = ggplot2::element_text(family = font, 
                                                                                                                                        size = 15, margin = ggplot2::margin(2, 0, 5, 0)), plot.caption = ggplot2::element_blank(), 
                 legend.position = "top", legend.text.align = 0, legend.background = ggplot2::element_blank(), 
                 legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), 
                 legend.text = ggplot2::element_text(family = font, size = 12, 
                                                     color = "#222222"), axis.title = ggplot2::element_blank(), axis.text.y = ggplot2::element_text(margin = ggplot2::margin(0,0,0,2)), 
                 axis.text = ggplot2::element_text(family = font, size = 10, 
                                                   color = "#222222"), 
axis.text.x = ggplot2::element_text(margin = ggplot2::margin(-5, b = 2)), axis.ticks = ggplot2::element_blank(), 
                 axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), 
                 panel.grid.major.y = ggplot2::element_line(color = "#cbcbcb"), 
                 panel.grid.major.x = ggplot2::element_blank(), panel.background = ggplot2::element_blank(), 
                 strip.background = ggplot2::element_rect(fill = "white"), 
                 strip.text = ggplot2::element_text(size = 10, hjust = 0))
}
```


```{r, warning = F, message = F, echo = F}
library(gridExtra)

valors = c(df_final$OWN_CAR_AGE,df_est$OWN_CAR_AGE, df_preprocessed_knn$OWN_CAR_AGE, df_preprocessed_mimmi$OWN_CAR_AGE, df_preprocessed_mice$OWN_CAR_AGE)

etiq = factor(rep(c("Original","Estadísticos", "kNN","MiMMi", "MICE"), each = 5000))

own_car_age = data.frame(valors, etiq)

p = ggplot(own_car_age, aes(valors, group = etiq, colour = etiq)) +
  geom_density(aes(fill = etiq), alpha = 0.5) + 
  geom_hline(yintercept = 0, size = 1, colour="#333333") +
  bbc_style() +
  scale_x_continuous(limits = c(0, 50),
                     breaks = seq(0, 50, by = 10),
                     labels = c("0", "10", "20", "30", "40", "50")) +
  theme(axis.title = element_text(size = 14),
        title = element_text(size = 70)) +
  labs(title = "Distribución de la variable OWN_CAR_AGE",
       subtitle = "Por los 4 métodos de imputación",
       x = "Años",
       y = "Densidad")


valors = c(df_final$AMT_GOODS_PRICE, df_est$AMT_GOODS_PRICE, df_preprocessed_knn$AMT_GOODS_PRICE, df_preprocessed_mimmi$AMT_GOODS_PRICE, df_preprocessed_mice$AMT_GOODS_PRICE)

etiq = factor(rep(c("Original", "Estadísticos", "kNN", "MiMMi", "MICE"), each = 5000))

goods_price = data.frame(valors, etiq)

g = ggplot(goods_price, aes(x = valors, group = etiq, colour = etiq)) +
  geom_density(aes(fill = etiq), alpha = 0.5) +
  geom_hline(yintercept = 0, size = 1, colour="#333333") +
  bbc_style() +
  scale_x_continuous(limits = c(0, 3500000),
                     breaks = seq(0, 3500000, by = 500000),
                     labels = c("0", "500000", "1000000", "1500000", "2000000", "2500000", "3000000", "3500000")) +
  theme(axis.title = element_text(size = 12),
        title = element_text(size = 18)) +
  labs(title = "Distribución de la variable AMT_GOODS_PRICE",
       subtitle = "Por los 4 métodos de imputación",
       x = "Cantidad (en $)",
       y = "Densidad")
```

**OWN_CAR_AGE**

Esta variable es la que presenta más valores no disponibles en nuestra base de datos, de forma que se acepta un mayor margen de error en cuanto a la imputación de valores se refiere. Así, la densidad resultante para cada método es la siguiente:

```{r, echo = F, warning = F}
p
```


Como se puede apreciar, hay tres métodos de imputación que claramente se alejan mucho de la distribución inicial de los datos: criterios estadísticos, kNN y MiMMi. Así pues, se puede apreciar como el MICE es el algoritmo que aproxima la densidad de los datos a los originales, de forma que este será el método escogido.

**AMT_GOODS_PRICE**

Como se ha visto previamente en al descriptiva preprocessing, esta variable únicamente presentaba 3 NA, de forma que la densidad en todos los métodos será muy similar:

```{r, warning = F, echo = F}
g
```

Como se puede apreciar, todos los métodos retornan una estimación similar de la densidad, por lo que se podría decir que es indiferente escoger un método en concreto. De esta forma, se decide usar el MICE como método de imputación final seleccionado.

He aquí una tabla resumen sobre los resultados obtenidos acerca de cuál es el mejor criterio de imputación:

|              | OWN_CAR_AGE | AMT_GOODS_PRICE |
|--------------|-------------|-----------------|
| Estadísticos |      No     |       Yes       |
| kNN          |      No     |       Yes       |
| MICE         |     Yes     |       Yes       |
| MiMMi        |      No     |       Yes       |

```{r, include = F}
df_preprocessed = df_preprocessed_mice
```



# Detección y tratamiento de outliers

En este apartado se tratará de visualizar aquellas observaciones extremas y, además, discernir sobre si deben ser corregidas o no, dependiendo de la naturaleza de la variable. Para ello, se utilizarán métodos multivariantes, como el análisis de componentes principales (PCA). Así, se procede a representar la proyección de los individuos en los primeros planos factoriales para así observar cuáles se alejan del resto de puntos: 


```{r, warning = F, message = F, echo = F}
library(dplyr)
library(FactoMineR)
library(factoextra)

df_prep_num = select_if(df_preprocessed, is.numeric)


pc1 = prcomp(df_prep_num, scale=TRUE)

pca_var = fviz_pca_var(pc1, repel = T)
pca_ind = fviz_pca_ind(pc1) +
  xlim(-20,20) +
  ylim(-6,6)

grid.arrange(pca_var, pca_ind, ncol = 2)

```

Como se puede apreciar, la combinación de las dos primeras dimensiones del PCA acumulan un total del 60% de la inercia total explicada, de forma que es un método de detección bastante fiable en nuestro caso. Identificamos, especialmente, un punto que sobresale del segundo plano factorial, mientras que podemos catalogar una decena de grupos realmente alejados del grupo en la primera dimensión:

```{r, echo = F}
pca_ind = fviz_pca_ind(pc1, label = "none") # hide individual labels

pca_ind + 
  stat_ellipse(level = 0.95, geom = "polygon", fill = "orange", alpha = 0.5, position = position_nudge(x = 8.5, y = 0))

```

Tras haber realizado el PCA correspondiente, vemos claramente un grupo de outliers. Así pues, pasamos a analizar los que son valores extremos por la dimensión 1. Como se puede apreciar, el primer plano factorial viene dado por las variables referidas a cantidad de dinero de nuestra base de datos. Así pues, los outliers presentes son personas con unos ingresos muy altos y que, además, realizaron préstamos por una cantidad de dinero muy superior al que cobran. Así pues, se trata de personas ricas, las cuales existen en nuestra sociedad, de forma que se quedan en la base de datos tal y como aparece. Más adelante, se aplicará alguna transformación que pueda permitir corregir estos valores tan extremos.

```{r, include = F}
name_outlier = which(pca_ind$data$x>8)

df_preprocessed[name_outlier,]
```

# Feature engineering

Por último, realizaremos la selección de variables final para nuestra base de datos, así como aplicar transformaciones correctas a nuestras variables para que cumplan algunas hipótesis, como normalidad o heteroscedasticidad. Para este apartado se hace una disección de cada variable una a una.

En primer lugar, se resolverán problemas relacionados con las variables numéricas. Como tenemos variables relacionadas con cantidades monetarias (salario, cantidad prestada...), tal vez sería mejor aplicar una transformación logarítmica: 

```{r, include = F}
df_preprocessed$log_AMT_INCOME_TOTAL = log(df_preprocessed$AMT_INCOME_TOTAL)
df_preprocessed$log_AMT_CREDIT = log(df_preprocessed$AMT_CREDIT)
df_preprocessed$log_AMT_ANNUITY = log(df_preprocessed$AMT_ANNUITY)
df_preprocessed$log_AMT_GOODS_PRICE = log(df_preprocessed$AMT_GOODS_PRICE)
```

Así pues, esta transformación debería resolver problemas relacionados con la normalidad de estas variables. Otro cambio a realizar es el respectivo a la variable `DAYS_BIRTH`, la cual muestra el número de días que lleva vivo el individuo. Sin embargo, el hecho de que esta variable esté en negativo y expresada en días (cuando normalmente se hace en años) hace que su interpretación sea complicada. De esta forma, se harán los cambios permanentes para encontrar la edad de los clientes, guardándola en una variable llamada `AGE_YEARS`.

```{r, include = F}
df_preprocessed$AGE_YEARS = floor(-df_preprocessed$DAYS_BIRTH/365)
```


Ahora, vamos a unir aquellas variables ya preprocesadas con el objetivo de tener el dataset preparado para crear nuevas variables.

```{r, include = F}
factor_vars = c("CODE_GENDER", "NAME_INCOME_TYPE", "NAME_EDUCATION_TYPE", 
                "NAME_FAMILY_STATUS", "OCCUPATION_TYPE", "ORGANIZATION_TYPE", 
                "REGION_RATING_CLIENT", "TARGET")

num_vars = c("log_AMT_INCOME_TOTAL", "log_AMT_CREDIT", "log_AMT_ANNUITY", "AGE_YEARS",
             "OWN_CAR_AGE", "log_AMT_GOODS_PRICE", "CNT_FAM_MEMBERS")

x = c(factor_vars, num_vars)
```

Antes de avanzar, haremos un correlograma para ver los pares de variables con un mayor coeficiente de correlación de Pearson:

```{r, echo = F}
corr_mat = cor(df_preprocessed[,num_vars])
corrplot(corr_mat)
```

Como se puede apreciar y como era de esperar, hay 3 variables que presentan una gran autocorrelación entre ellas: `log_AMT_CREDIT`, `log_AMT_GOODS_PRICE` y `log_AMT_ANNUITY`. de esta forma, sería ideal nuevas variables a partir de éstas con las cuales se pueda resolver este problema, ya que explican exactamente lo mismo. Para ello, será necesario basarse en la teoría económica y en qué se fijan las entidades de crédito para conceder préstamos. Así, el siguiente objetivo será crear ratios y variables que pretendan controlar y relacionar dinero prestado con capacidad del cliente para retornarlo:

-   DIFF_CREDIT_GOODS: Diferencia entre el crédito pedido y el valor del bien para el que se quiere usar
-   RATIO_CREDIT_INCOME: Ratio entre el crédito pedido y el salario anual del prestatario. También se puede contar como el número de años que se tarda en devolver el crédito
-   RATIO_ANNUITY_CREDIT: Ratio entre la anuidad del préstamo y el crédito total solicitado
-   DTI_RATIO: El DTI (Debt-to-income) ratio mide la capacidad del cliente para pagar la annuity de su préstamo en relación con sus ingresos

```{r, echo = F}
df_preprocessed$DIFF_CREDIT_GOODS = df_preprocessed$AMT_CREDIT - df_preprocessed$AMT_GOODS_PRICE
df_preprocessed$RATIO_CREDIT_INCOME = df_preprocessed$AMT_CREDIT / df_preprocessed$AMT_INCOME_TOTAL
df_preprocessed$RATIO_ANNUITY_CREDIT = df_preprocessed$AMT_ANNUITY / df_preprocessed$AMT_CREDIT
df_preprocessed$DTI_RATIO = df_preprocessed$AMT_ANNUITY / df_preprocessed$AMT_INCOME_TOTAL



num_vars = c("log_AMT_INCOME_TOTAL", "log_AMT_CREDIT", "log_AMT_ANNUITY", "AGE_YEARS",
             "OWN_CAR_AGE", "log_AMT_GOODS_PRICE", "CNT_FAM_MEMBERS", "DIFF_CREDIT_GOODS",
             "RATIO_CREDIT_INCOME", "RATIO_ANNUITY_CREDIT", "DTI_RATIO")

corr_mat = cor(df_preprocessed[,num_vars])
corrplot(corr_mat)
```

Se puede apreciar que, ahora, las nuevas variables creadas no presentan tanta correlación entre ellas como anteriormente había. Se puede apreciar, además, que las correlaciones entre las variables donde había problemas siguen teniéndolos y, como se aprecia en el PCA sencillo realizado antes, será necesario descartar alguna variable, ya que explican cosas similares en las mismas dimensiones. Así, en el PCA se deberá realizar el descarte adecuado de variables en función de su aportación al PCA resultante.


## Eliminación de variables a través del PCA

Se proceden a eliminar, primeramente, aquellas variables para las cuales ya existe su transformación logarítmica. Esto se hace para no contar con variables que contengan la misma capacidad explicativa (y así evitar colinealidad). También se elimina la variable DAYS_BIRTH, ya que se cuenta con AGE_YEARS, que es una transformación de la inicial, debido a que DAYS_BIRTH no tenia una clara interpretación.

```{r, echo=F, include=FALSE}
# Eliminamos las variables a las que se les ha realizado la transformación log

data = df_preprocessed

data <- data[ , -which(names(data) %in% c("AMT_INCOME_TOTAL","AMT_CREDIT","AMT_ANNUITY","AMT_GOODS_PRICE","DAYS_BIRTH"))]
#Eliminamos DAYS_BIRTH, ya que contamos con la variable AGE_YEARS

# Nos quedamos sólo con los datos numéricos
numeric <- which(sapply(data, is.numeric))
data_numeric <- data[, numeric]
# sapply(data_numeric, class)
```


```{r, echo=FALSE, include=FALSE}
pc1 <- prcomp(data_numeric, scale=TRUE)
inerProj<- pc1$sdev^2 
totalIner<- sum(inerProj)
pinerEix<- 100*inerProj/totalIner
percInerAccum<-100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2]
df <- data.frame(x=1:length(percInerAccum),
                 y=(100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2])/(100/40))
```

```{r, echo=F, include = F}
p <- fviz_eig(pc1, addlabels = TRUE,ylim=c(0,40),barcolor = "#53868B", barfill = "#DCF0F8")
p <- p + 
     geom_point(data=df, aes(x,y), size=2, color="#8B3E2F") +
     geom_line(data=df, aes(x,y), color="#8B3E2F") +
     scale_y_continuous(sec.axis = sec_axis(~ . * (100/40), 
                                   name = "Cumulative proportion of variance explained (%)")) +
    geom_text(data = df, aes(x,y-1.5,label=paste(round((y*100/40),1),"%"))) +
  ggtitle("Explained inertia by each dimension") +
  theme_minimal()
```

```{r, echo = F, fig.cap="Porcentaje de inercia explicado por dimensión", fig.show='hold', out.height="75%", out.width="75%"}
p
```

Teniendo en cuenta que la inercia equivale a la proporción de la variabilidad de los datos, se sabe que con un 80% de inercia se puede obtener casi toda la información o variabilidad de la base de datos original. Con ello, vemos que el 80% de la inercia acumulada se logra con 5 planos factoriales, pero aún se pueden eliminar algunas variables.

```{r, echo=F, include = F}
nd <- which(percInerAccum >= 80)[1]
Psi <- pc1$x[,1:nd]
iden <- row.names(data_numeric) # Etiquetas de los individuos
etiq <- names(data_numeric) # Etiquetas de variables numéricas
ze <- rep(0,length(etiq)) # Vector necesario para realizar gráficos posteriores
# dim(Psi)
```

```{r, echo = F, include = F}
# En el objeto "combn" se guardan todos los pares de dimensiones a graficar
combn <- as.data.frame(t(combn(1:nd,2))) 
grafic = c()
for(i in 1:nrow(combn)){
  grafic[[i]] = fviz_pca_var(pc1,axes = c(combn[i,1],combn[i,2]),col.var = "contrib",
                             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                             repel = TRUE) + theme_minimal()
}
```

```{r, echo = F, fig.cap="Proyección de variables en los dos primeros planos factoriales", fig.show='hold', out.height="75%", out.width="75%"}
grafic[[1]]
```

```{r, out.width="90%", include=FALSE}
# COMENTADO PORQUE DA EL MISMO GRAFICO ANTERIOR

# eje1 <- 1
# eje2 <- 2
# Phi = cor(data_numeric, Psi)
# X <- Phi[, eje1]
# Y <- Phi[, eje2]
# plot(Psi[,eje1],Psi[,eje2],type="n",xlim=c(min(X,0),max(X,0)), ylim=c(-1,1), 
#      xlab = "CP1", ylab = "CP2") 
# axis(side=1, pos= 0, labels = F)
# axis(side=3, pos= 0, labels = F)
# axis(side=2, pos= 0, labels = F)
# axis(side=4, pos= 0, labels = F)
# arrows(ze, ze, X, Y, length = 0.07,col="blue")
# text(X,Y,labels=etiq,col="darkblue", cex=0.7)
```

Observamos la tabla de rotaciones:

```{r, echo=FALSE}
kbl(pc1$rotation[,1:5],
    caption = "Correlación de cada variable con cada plano factorial"
    , booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

En el grafico vemos que las flechas de **log_AMT_GOODS_PRICE** y **log_AMT_CREDIT** se solapan entre ellas, eso queire decir que las dos variables explican el mismo plano factorial. Vemos en la tabla de rotaciones que **log_AMT_CREDIT** contribuye más a explicar el primer plano factorial, y además las correlaciones entra cada una de las variables y cada dimensión son muy similares. Por esta razón eliminamos **log_AMT_GOODS_PRICE**.

```{r, echo=FALSE, include=FALSE}
data_numeric <- data_numeric[ , -which(names(data_numeric) %in% c("log_AMT_GOODS_PRICE"))]
pc1 <- prcomp(data_numeric, scale = TRUE)
```

Nos quedamos con una variable menos, por tanto tenemos `r ncol(data_numeric)` variables numéricas. 

De vuelta, verificamos el porcentaje de inercia por cada componente principal y la acumulada:
```{r, echo=F, include = F}
inerProj<- pc1$sdev^2 
totalIner<- sum(inerProj)
pinerEix<- 100*inerProj/totalIner
percInerAccum<-100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2]
df <- data.frame(x=1:length(percInerAccum), y=(100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2])/(100/percInerAccum[1]))
```

```{r, echo=F, include = F}
p <- fviz_eig(pc1, addlabels = TRUE,ylim=c(0,percInerAccum[1]),barcolor = "#53868B", barfill = "#DCF0F8")
p <- p + 
     geom_point(data=df, aes(x,y), size=2, color="#8B3E2F") +
     geom_line(data=df, aes(x,y), color="#8B3E2F") +
     scale_y_continuous(sec.axis = sec_axis(~ . * (100/percInerAccum[1]), 
                                   name = "Cumulative proportion of variance explained (%)")) +
    geom_text(data = df, aes(x,y-1.5,label=paste(round((y*100/percInerAccum[1]),1),"%"))) +
  ggtitle("Explained inertia by each dimension") + theme_minimal()
```

```{r, echo = F, fig.cap="Porcentaje de inercia explicado por dimensión", fig.show='hold', out.height="75%", out.width="75%"}
p
```

Como se puede ver, seguimos teniendo 5 dimensiones que acumulan el 80% de la varianza.

```{r, echo=F, include = F}
nd <- which(percInerAccum >= 80)[1]
Psi <- pc1$x[,1:nd]
iden <- row.names(data_numeric)
etiq <- names(data_numeric)
ze <- rep(0,length(etiq))
Phi <- cor(data_numeric,Psi)
```

```{r, echo = FALSE, include = FALSE}
combn <- as.data.frame(t(combn(1:nd,2))) 
grafic = c()
for(i in 1:nrow(combn)){
  grafic[[i]] = fviz_pca_var(pc1,axes = c(combn[i,1],combn[i,2]),col.var = "contrib",
                             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                             repel = TRUE) + theme_minimal()
}
```

```{r,echo = F, fig.cap="Proyección de variables en los dos primeros planos facoriales", fig.show='hold', out.height="75%", out.width="75%"}
grafic[[1]]
```

Vemos que las variables **CNT_FAM_MEMBERS**, **AGE_YEARS** y **OWN CAR AGE** no explican las dos primeras componentes pero si nos fijamos en la tabla de rotaciones vemos que sí tienen importancia a la hora de explicar las otras tres dimensiones:

```{r, echo=FALSE}
kbl(pc1$rotation[,1:5],
    caption = "Correlación de cada variable con cada plano factorial"
    , booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Por ejemplo, en el caso de **OWN CAR AGE** se puede ver en la tabla anterior que, se podría decir que no es la que mejor explica las primeras componentes, pero vemos que explica casi toda la componente 5.

Otra observación se podria hacer de las variables **log_AMT_CREDIT** y **log_AMT_ANNUITY**, donde se puede apreciar que tienen correlaciones similares con la primera y segunda dimensión. Teniendo en cuenta que esas dos primeras dimensiones (PC1 y PC2) són las más importantes, ya que acumulan la mayoría de la inercia (en total un 52.2%), parece una decisión sensata eliminar una de ellas, en este caso **log_AMT_ANNUITY**.

```{r, echo=FALSE, include=FALSE}
data_numeric <- data_numeric[ , -which(names(data_numeric) %in% c("log_AMT_ANNUITY"))]
pc1 <- prcomp(data_numeric, scale = TRUE)
```

Ahora conservamos `r ncol(data_numeric)` variables numéricas.

De forma igual que anteriormente, comprobamos el porcentaje de inercia para cada componente principal y la acumulada:
```{r, echo=F, include = F}
inerProj<- pc1$sdev^2 
totalIner<- sum(inerProj)
pinerEix<- 100*inerProj/totalIner
percInerAccum<-100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2]
df <- data.frame(x=1:length(percInerAccum), y=(100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2])/(100/percInerAccum[1]))
```

```{r, echo=F, include = F}
p <- fviz_eig(pc1, addlabels = TRUE,ylim=c(0,percInerAccum[1]),barcolor = "#53868B", barfill = "#DCF0F8")
p <- p + 
     geom_point(data=df, aes(x,y), size=2, color="#8B3E2F") +
     geom_line(data=df, aes(x,y), color="#8B3E2F") +
     scale_y_continuous(sec.axis = sec_axis(~ . * (100/percInerAccum[1]), 
                                   name = "Cumulative proportion of variance explained (%)")) +
    geom_text(data = df, aes(x,y-1.5,label=paste(round((y*100/percInerAccum[1]),1),"%"))) +
  ggtitle("Explained inertia by each dimension") + theme_minimal()
```

```{r, echo = F, fig.cap="Porcentaje de inercia explicado por dimensión", fig.show='hold', out.height="75%", out.width="75%"}
p
```

Como se puede comprobar, las 5 dimensiones siguen siendo las necesarias para acumular el 80% de la varianza.

```{r, echo=F, include = F}
nd <- which(percInerAccum >= 80)[1]
Psi <- pc1$x[,1:nd]
iden <- row.names(data_numeric)
etiq <- names(data_numeric)
ze <- rep(0,length(etiq))
Phi <- cor(data_numeric,Psi)
```

```{r, echo = F, include = FALSE}
combn <- as.data.frame(t(combn(1:nd,2))) 
grafic = c()
for(i in 1:nrow(combn)){
  grafic[[i]] = fviz_pca_var(pc1,axes = c(combn[i,1],combn[i,2]),col.var = "contrib",
                             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                             repel = TRUE) + theme_minimal()
}
```

```{r,echo = F, fig.cap="Proyección de variables en los dos primeros planos factoriales", fig.show='hold', out.height="75%", out.width="75%"}
grafic[[1]]
```

Observamos tambien la tabla de rotaciones para verificar si se puede eliminar alguna variable más:

```{r, echo=FALSE}
kbl(pc1$rotation[,1:5],
    caption = "Correlación de cada variable con cada plano factorial"
    , booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Si nos fijamos en el gráfico que incluye los dos primeros planos factoriales (PC1 y PC2), resulta fácil ver que **log_AMT_CREDIT** y **DIFF_CREDIT_GOODS** se solapan en su proyección, teniendo **log_AMT_CREDIT** más contribución dado que el vector es más largo. De aquí se entiende que las correlaciones de ambas variables en los dos primeros planos factoriales son muy similares, motivo por el cual solapan. En la tabla de correlaciones anterior se puede comprobar como efectivamente, estas correlaciones son similares. Incluso la correlación en ambas variables con la tercera dimensión (PC3) es baja, de forma parecida. Por tanto, se procede a eliminar aquella con menos contribución en PC1 y PC2, esta siendo **DIFF_CREDIT_GOODS**.

```{r, echo=FALSE, include=FALSE}
data_numeric <- data_numeric[ , -which(names(data_numeric) %in% c("DIFF_CREDIT_GOODS"))]
pc1 <- prcomp(data_numeric, scale = TRUE)
```

Ahora se conservan `r ncol(data_numeric)` variables numéricas.

Se vuelven a ejecutar todos los pasos anteriores para volver a verificar si hace falta eliminar más variables:

```{r, echo=F, include = F}
inerProj<- pc1$sdev^2 
totalIner<- sum(inerProj)
pinerEix<- 100*inerProj/totalIner
percInerAccum<-100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2]
df <- data.frame(x=1:length(percInerAccum), y=(100*cumsum(pc1$sdev[1:dim(data_numeric)[2]]^2)/dim(data_numeric)[2])/(100/percInerAccum[1]))
```

```{r, echo=F, include = F}
p <- fviz_eig(pc1, addlabels = TRUE,ylim=c(0,percInerAccum[1]),barcolor = "#53868B", barfill = "#DCF0F8")
p <- p + 
     geom_point(data=df, aes(x,y), size=2, color="#8B3E2F") +
     geom_line(data=df, aes(x,y), color="#8B3E2F") +
     scale_y_continuous(sec.axis = sec_axis(~ . * (100/percInerAccum[1]), 
                                   name = "Cumulative proportion of variance explained (%)")) +
    geom_text(data = df, aes(x,y-1.5,label=paste(round((y*100/percInerAccum[1]),1),"%"))) +
  ggtitle("Explained inertia by each dimension") + theme_minimal()
```

```{r, echo = F, fig.cap="Porcentaje de inercia explicado por dimensión", fig.show='hold', out.height="75%", out.width="75%"}
p
```

Se aprecia como la eliminación de **DIFF_CREDIT_GOODS** ha modificado el número de dimensiones necesarias para alcanzar el 80% de inercia acumulada, pasando de 5 a 4 dimensiones.

```{r, echo=F, include = F}
nd <- which(percInerAccum >= 80)[1]
Psi <- pc1$x[,1:nd]
iden <- row.names(data_numeric)
etiq <- names(data_numeric)
ze <- rep(0,length(etiq))
Phi <- cor(data_numeric,Psi)
```

```{r, echo = F, include = FALSE}
combn <- as.data.frame(t(combn(1:nd,2))) 
grafic = c()
for(i in 1:nrow(combn)){
  grafic[[i]] = fviz_pca_var(pc1,axes = c(combn[i,1],combn[i,2]),col.var = "contrib",
                             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
                             repel = TRUE) + theme_minimal()
}
```

```{r, echo = F, fig.cap="Proyección de variables en los dos primeros planos factoriales", fig.show='hold', out.height="75%", out.width="75%"}
grafic[[1]]
```

```{r, echo=FALSE}
kbl(pc1$rotation[,1:4],
    caption = "Correlación de cada variable con cada plano factorial"
    , booktabs=T)%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```

Comprobando el gráfico de las dos primeras dimensiones, y analizando las correlaciones, parece ser que ya no hace falta eliminar más variables. Por tanto, conservamos `r ncol(data_numeric)` variables numéricas.

Las variables eliminadas han sido:
- **AMT_INCOME_TOTAL**, **AMT_CREDIT**, **AMT_ANNUITY**, **AMT_GOODS_PRICE**, todas ellas con motivo de que ya se habia creado otra variable a partir de su transformación logarítmica.
- **DAYS_BIRTH**, ya que la variable **AGE_YEARS** es una transformación de ella.
- **log_AMT_GOODS_PRICE**
- **log_AMT_ANNUITY**
- **DIFF_CREDIT_GOODS**



# Balanceo de los datos

```{r}
library(ROSE)

data_factor = select_if(data, is.factor)
data_desbalanceada = data.frame(data_factor, data_numeric)

x = names(data_desbalanceada)[!(names(data_desbalanceada) %in% c("TARGET"))]
y = "TARGET"


data_balanceada = ovun.sample(reformulate(x,y), data = data_desbalanceada, method = "both", p = 0.5, seed = 123)$data

summary(data_balanceada)
```

```{r}
summary(data_desbalanceada)
summary(data_balanceada)
```


```{r, include = F, eval = F}
rm(list=setdiff(ls(), c("data_balanceada", "data_desbalanceada")))
save.image(file = "Dades noves (balancejada i no balancejada).RData")



```