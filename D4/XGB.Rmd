---
title: "Untitled"
author: "Oscar Arroyo"
date: "2023-12-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

XGBoost es un modelo basado en árboles de decisión y es una mejora de otros métodos de ensamblaje como el Random Forest. El algoritmo utiliza varios métodos de optimización para mejorar la precisión y controlar el sobreajuste.

El proceso de XGBoost comienza con una predicción inicial y luego calcula los residuos, que son las diferencias entre las predicciones y los valores observados. Luego, crea un árbol de decisión con estos residuales y continúa este proceso, construyendo árboles secuenciales que aprenden de los errores del árbol anterior.

Este modelo se puede ajustar según ciertos parámetros. Para encontrar la mejor combinación de parámetros, se llevará a cabo validación cruzada en el conjunto de entrenamiento con el fin de extraer los mejores hiperparámetros. Posteriormente, se aplicará el mejor modelo obtenido en nuestro conjunto de prueba para su validación.

```{r, include=FALSE}
library(caret)
library(dplyr)
library(xgboost)
library(kableExtra)
```


```{r}
load("Dades noves (balancejada i no balancejada).Rdata")

Train <- train_balanceado

TARGET <- as.vector(Train$TARGET)
Train$TARGET <- NULL #TARGET tiene q ser factor 2 niveles, no numeric

# Objectos <- sapply(Train, class)
# Categoriques <- names(Objectos)[which(Objectos%in%c("factor"))]
# 
# Cat <- predict(dummyVars("~ .", data = Train[,Categoriques]), newdata = Train[,Categoriques])
# 
# Train[,Categoriques] <- NULL
# 
# Train <- cbind(Train, Cat)

Train <- Train %>%
  mutate_if(is.factor, as.numeric)

Train$TARGET <- TARGET

grid <- expand.grid(
  nrounds = c(100), # Número de rondas de boosting
  max_depth = seq(3,15), # Profundidad máxima de los árboles
  eta = seq(0.01, 0.01, 0.001), # Tasa de aprendizaje
  gamma = c(0), # Parámetro de regularización
  colsample_bytree = c(1), # Fracción de columnas para cada árbol
  min_child_weight = c(1), # Suma mínima de pesos de las instancias necesarias en un hijo
  subsample = c(1) # Fracción de instancias para entrenar cada árbol
)

control <- trainControl(method = "cv", number = 10, verboseIter = FALSE)

model <- train(
  TARGET ~ ., data = Train,
  method = "xgbTree",
  trControl = control,
  tuneGrid = grid,
  verbosity = 0
)
model$bestTune[,c(2,3)]
model$resample
```

redactar cuales son los mejores params

```{r}
Train <- train_balanceado
Test <- validation_balanceado

X_train <- Train %>% select(-"TARGET")
y_train <- Train[["TARGET"]]
X_test <- Test %>% select(-"TARGET")
y_test <- Test[["TARGET"]]

X_train <- X_train %>%
  mutate_if(is.factor, as.numeric)
X_test <- X_test %>%
  mutate_if(is.factor, as.numeric)

dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_test), label = y_test)

params <- list(
  objective = "binary:hinge",
  eval_metric = "error", 
  nrounds = 100,  
  early_stopping_rounds = 10, 
  verbose = FALSE,  
  eta= 0.01,
  max_depth=15
)

xgb_model <- xgb.train(params = params, data = dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest))
xgboost()
xgb.plot.tree(xgb_model, trees = 100)

xgb.plot.tree()
```

```{r}
predictions <- predict(xgb_model, newdata = dtest)

conf_matrix <- confusionMatrix(as.factor(predictions),y_test,positive = "1")

accuracy <- conf_matrix$overall["Accuracy"]
specificity <- conf_matrix$byClass["Specificity"]
sensitivity <- conf_matrix$byClass["Sensitivity"]
f1_score <- conf_matrix$byClass["F1"]

metricas <- data.frame(Accuracy=accuracy,Specificity=specificity,Sensitivity=sensitivity,F1=f1_score)
metricas <- as.matrix(metricas)
kbl(metricas, 
    caption = "Métricas de validación", align = "c")%>% 
  kable_styling(position = "center", 
                latex_options = c("HOLD_position"))
```
resultados y conclusiones
